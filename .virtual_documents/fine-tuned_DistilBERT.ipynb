


# Install the required libraries if not already installed
# !pip install transformers torch datasets
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
# Load the fine-tuned DistilBERT model and tokenizer
# MODEL_NAME = "shivgan3/shivgan-fine-tuned-model-name"  # Replace with your fine-tuned model's name or path
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,  model_type="bert")
# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)


model_name = "distilbert-base-uncased"  # Or use any other model from the Hugging Face Model Hub
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)


# Define the text inputs for classification
texts = [
    "The stock market is showing signs of recovery.",
    "Unexpected economic downturn affects financial stability.",
    "Major company files for bankruptcy."
]

# Tokenize the inputs
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

# Define label mapping (adjust this to match your fine-tuned model's labels)
label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}

# Map predictions to labels
classified_labels = [label_map[pred.item()] for pred in predictions]

# Print results
for text, label in zip(texts, classified_labels):
    print(f"Text: \"{text}\"\nPredicted Sentiment: {label}\n")



from transformers import AutoTokenizer

# Replace with the correct model or base model name
model_name = "shivgan3/shivgan-fine-tuned-model-name"

try:
    # Attempt to load tokenizer from the given model name
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print("Tokenizer loaded successfully!")
except OSError as e:
    print(f"Error: {e}")
    print("Attempting to use a base model tokenizer as fallback...")

    # Fallback to a base BERT tokenizer if the custom tokenizer cannot be loaded
    base_model = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    print(f"Fallback to base model tokenizer ({base_model}) successful!")

# Test the tokenizer
test_sentence = "This is a test sentence for the tokenizer."
tokenized_output = tokenizer.tokenize(test_sentence)
print("Tokenized output:", tokenized_output)



# Install the required libraries if not already installed
# !pip install transformers torch datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import os

# Set local directory for saving the model and tokenizer
MODEL_DIR = "./shivgan3_shivgan_fine_tuned_model"  # Path to your local model directory

# Ensure the model and tokenizer are downloaded
if not os.path.exists(MODEL_DIR):
    print(f"Model directory {MODEL_DIR} does not exist. Please download and save the model from Hugging Face manually.")
else:
    # Load the fine-tuned model and tokenizer from local files
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)

    # Define the text inputs for classification
    texts = [
        "The stock market is showing signs of recovery.",
        "Unexpected economic downturn affects financial stability.",
        "Major company files for bankruptcy."
    ]

    # Tokenize the inputs
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

    # Perform inference
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=-1)

    # Define label mapping (adjust this to match your fine-tuned model's labels)
    label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}

    # Map predictions to labels
    classified_labels = [label_map[pred.item()] for pred in predictions]

    # Print results
    for text, label in zip(texts, classified_labels):
        print(f"Text: \"{text}\"\nPredicted Sentiment: {label}\n")



# Install the required libraries if not already installed
# !pip install transformers torch datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import os

# Define the model name (pre-trained model of your choice)
MODEL_NAME = "distilbert-base-uncased"  # Replace with your chosen pre-trained model

# Set the local directory for saving the model and tokenizer
MODEL_DIR = "./my_local_model"  # Path to save model locally

# Step 1: Download and save the model and tokenizer locally if not already saved
if not os.path.exists(MODEL_DIR):
    print(f"Downloading and saving the model to {MODEL_DIR}...")
    # Load the tokenizer and model from Hugging Face
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

    # Save the model and tokenizer locally
    tokenizer.save_pretrained(MODEL_DIR)
    model.save_pretrained(MODEL_DIR)
    print(f"Model saved to {MODEL_DIR}")
else:
    # Step 2: Load the model and tokenizer from the local directory
    print(f"Loading model from local directory {MODEL_DIR}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)

# Define the text inputs for classification
texts = [
    "The stock market is showing signs of recovery.",
    "Unexpected economic downturn affects financial stability.",
    "Major company files for bankruptcy."
]

# Tokenize the inputs
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

# Define label mapping (adjust this to match your fine-tuned model's labels)
label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}  # Adjust based on your model's labels

# Map predictions to labels
classified_labels = [label_map[pred.item()] for pred in predictions]

# Print results
for text, label in zip(texts, classified_labels):
    print(f"Text: \"{text}\"\nPredicted Sentiment: {label}\n")




