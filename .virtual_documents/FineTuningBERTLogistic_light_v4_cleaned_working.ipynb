#Fine-Tuning BERT with a Logistic Regression Layer
import torch
from torch import nn
from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification
from datasets import load_dataset
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


dataset = load_dataset("imdb")


dataset


import torch
from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification
from datasets import load_dataset
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Load IMDB dataset
dataset = load_dataset("imdb")

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Balance the dataset to include both classes
def balance_classes(dataset, num_samples):
    class_0 = [sample for sample in dataset if sample["label"] == 0][:num_samples // 2]
    class_1 = [sample for sample in dataset if sample["label"] == 1][:num_samples // 2]
    return class_0 + class_1

balanced_train_data = balance_classes(tokenized_datasets["train"], 100)
balanced_eval_data = balance_classes(tokenized_datasets["test"], 50)

# Define the training arguments with optimizations
training_args = TrainingArguments(
    output_dir="./results",                 # Output directory
    evaluation_strategy="no",               # Disable evaluation during training
    learning_rate=2e-5,                     # Learning rate
    per_device_train_batch_size=4,          # Smaller batch size for faster training
    per_device_eval_batch_size=4,           # Smaller batch size for evaluation
    num_train_epochs=1,                     # Reduce number of epochs to 1
    fp16=True,                              # Enable mixed precision training
    max_steps=100,                          # Limit steps for faster execution
)

# Load the model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Function to extract embeddings (logits) from BERT
def extract_embeddings(model, dataset):
    embeddings = []
    labels = []
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for sample in dataset:  # Loop through each item in the dataset
            input_ids = torch.tensor(sample["input_ids"]).unsqueeze(0)  # Add batch dimension
            attention_mask = torch.tensor(sample["attention_mask"]).unsqueeze(0)  # Add batch dimension
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits.cpu().numpy()
            embeddings.append(logits.flatten())  # Append flattened logits
            labels.append(sample["label"])  # Append label
    return np.array(embeddings), np.array(labels)

# Extract embeddings from the train and test datasets
train_embeddings, train_labels = extract_embeddings(model, balanced_train_data)
test_embeddings, test_labels = extract_embeddings(model, balanced_eval_data)

# Train a logistic regression model on top of the embeddings
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(train_embeddings, train_labels)

# Predict and evaluate
train_preds = log_reg.predict(train_embeddings)
test_preds = log_reg.predict(test_embeddings)

train_accuracy = accuracy_score(train_labels, train_preds)
test_accuracy = accuracy_score(test_labels, test_preds)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")

# Initialize the Trainer for BERT
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=balanced_train_data,
    eval_dataset=balanced_eval_data,
)

# Train the model (BERT fine-tuning)
trainer.train()

# Save the trained BERT model and tokenizer for future use
model.save_pretrained("./trained_model")
tokenizer.save_pretrained("./trained_model")

# Evaluate the fine-tuned model on the test set
trainer.evaluate()

# Predict using the fine-tuned BERT model
def predict(texts, model, tokenizer):
    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**encodings)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
    return predictions.numpy()

# Example prediction
texts = ["I love this movie!", "This movie was terrible."]
predictions = predict(texts, model, tokenizer)
print("Predictions:", predictions)  # Output will be the predicted labels (0 or 1)






predictions


texts


model


tokenizer


# Predict using the fine-tuned BERT model
def predict(texts, model, tokenizer):
    # Encode the input texts using the tokenizer
    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=128)
    
    # Ensure the model is in evaluation mode
    model.eval()

    with torch.no_grad():
        # Forward pass through the model to get logits
        outputs = model(**encodings)
        logits = outputs.logits
        
        # Convert logits to probabilities using softmax (optional but useful for multi-class)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        
        # Get the predicted class (index of the highest probability)
        predictions = torch.argmax(probs, dim=-1)
    
    return predictions.numpy()

# Example prediction
texts = ["I love this movie!", "This movie was terrible."]
predictions = predict(texts, model, tokenizer)
print("Predictions:", predictions)  # Output should show the predicted labels (0 or 1)



# Example prediction
texts = [
    "I love this movie! It's amazing and so entertaining.",
    "This movie was terrible. I hated every minute of it.",
    "The plot was great, but the acting could have been better.",
    "A fantastic film with breathtaking visuals and a captivating story!",
    "I was really bored throughout this film. It's not worth watching.",
    "One of the best movies I’ve seen this year. I highly recommend it!",
    "The movie started off strong but ended poorly.",
    "Not bad, but not great either. Just an average movie.",
    "I couldn't stop laughing at the jokes. This was a really funny film.",
    "It was too predictable and lacked depth. Would not watch again.",
    "What a disappointment! The trailer was way better than the actual movie.",
    "I’m glad I watched it, but I wouldn’t watch it again.",
    "Incredible! A masterpiece that will be remembered for years.",
    "This movie is a must-see for any fan of action films.",
    "Absolutely awful. I don’t understand the hype around it.",
    "The soundtrack was amazing, and the acting was top-notch.",
    "This was a complete waste of time. I would not recommend it to anyone.",
    "The movie was too long and dragged on. Could have been much shorter.",
    "A rollercoaster of emotions. I loved every second of it.",
    "It’s a fun movie to watch with friends, but not the best out there.",
    "A very emotional and touching story that left me in tears.",
    "The special effects were spectacular, but the story was lacking.",
    "I didn’t connect with the characters at all, but the film was well made.",
    "If you're looking for something lighthearted, this movie is perfect.",
    "A complete masterpiece from start to finish. I loved everything about it!",
    "This movie is definitely overrated. I don’t see what the fuss is about."
]

# Predictions
predictions = predict(texts, model, tokenizer)
print("Predictions:", predictions)  # Output will be the predicted labels (0 or 1)



import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from datasets import load_dataset
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score
import numpy as np

# 1. Load a smaller dataset (a subset of IMDB for faster processing)
dataset = load_dataset("imdb", split='train[:5%]')  # Use only 5% of the data for faster training
test_dataset = load_dataset("imdb", split='test[:5%]')  # Use only 5% of the test data

# 2. Load a smaller model (DistilBERT)
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# 3. Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=64)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_test_data = test_dataset.map(tokenize_function, batched=True)

# 4. Extract embeddings (features) from DistilBERT
def extract_embeddings(model, dataset):
    embeddings = []
    labels = []
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for sample in dataset:
            input_ids = torch.tensor(sample["input_ids"]).unsqueeze(0)  # Add batch dimension
            attention_mask = torch.tensor(sample["attention_mask"]).unsqueeze(0)  # Add batch dimension
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits.cpu().numpy()
            embeddings.append(logits.flatten())  # Append flattened logits
            labels.append(sample["label"])  # Append label
    return np.array(embeddings), np.array(labels)

# Extract embeddings from train and test datasets
train_embeddings, train_labels = extract_embeddings(model, tokenized_datasets)
test_embeddings, test_labels = extract_embeddings(model, tokenized_test_data)

# 5. Ensure both classes are present before applying feature selection
# Check if there are both classes in the data
unique_train_labels = np.unique(train_labels)
if len(unique_train_labels) < 2:
    raise ValueError("The training data contains samples from only one class.")

# 6. Apply feature selection (SelectKBest)
# Select top 50 features for faster processing
selector = SelectKBest(score_func=f_classif, k=50)
train_embeddings_selected = selector.fit_transform(train_embeddings, train_labels)
test_embeddings_selected = selector.transform(test_embeddings)

# 7. Identify the selected and removed features
selected_features = selector.get_support(indices=True)  # Indices of selected features
removed_features = [i for i in range(train_embeddings.shape[1]) if i not in selected_features]

# Display the selected and removed features
print("Selected feature indices:", selected_features)
print("Removed feature indices:", removed_features)

# 8. Train a Logistic Regression model on the selected features
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(train_embeddings_selected, train_labels)

# 9. Evaluate the model
train_preds = log_reg.predict(train_embeddings_selected)
test_preds = log_reg.predict(test_embeddings_selected)

train_accuracy = accuracy_score(train_labels, train_preds)
test_accuracy = accuracy_score(test_labels, test_preds)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")




