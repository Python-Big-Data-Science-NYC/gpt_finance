





try:
    # Attempt to access the variable
    path = path
except NameError:
    # If the variable is not defined, set it to None
    path = None


# Import libraries. begin, let's import the necessary libraries that we'll be using throughout this notebook:

# Data Manipulation Libraries
import numpy as np 
import pandas as pd

# Data Visualization Libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Machine Learning Libraries
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_log_error, make_scorer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import TomekLinks


import os
full_path_files=[]
if path is None:
    for dirname, _, filenames in os.walk('/home/j/.cache/kagglehub/datasets/saurabhbagchi/dish-network-hackathon/versions/1'):
        for filename in filenames:
            print(os.path.join(dirname, filename))
            full_path_files.append(os.path.join(dirname, filename))
else:
    for dirname, _, filenames in os.walk(path):
        for filename in filenames:
            print(os.path.join(dirname, filename))
            full_path_files.append(os.path.join(dirname, filename))


full_path_files


for filename in full_path_files:
    if 'Train_Dataset' in filename:
        Train_Dataset=filename
        


Train_Dataset





df = pd.read_csv(Train_Dataset)
pd.set_option('display.max_columns', None)
df.head()


# List of column names representing numerical data features

numerical_data = [["ID", "Client_Income", "Car_Owned", "Bike_Owned", "Active_Loan", "House_Own", "Child_Count",
                  "Credit_Amount", "Loan_Annuity", "Population_Region_Relative", "Age_Days", "Employed_Days", "Registration_Days",
                  "ID_Days", "Own_House_Age", "Mobile_Tag","Homephone_Tag", "Workphone_Working", "Client_Family_Members", 
                   "Cleint_City_Rating", "Application_Process_Day", "Application_Process_Hour", "Score_Source_1",
                   "Score_Source_2", "Score_Source_3", "Social_Circle_Default", "Phone_Change", "Credit_Bureau", "Default"]]

# List of column names representing categorical data features
categorical_data = ["Accompany_Client", "Client_Income_Type", "Client_Education", "Client_Marital_Status", "Client_Gender",
                    "Loan_Contract_Type", "Client_Housing_Type", "Client_Occupation", "Client_Permanent_Match_Tag", 
                     "Client_Contact_Work_Tag", 
                     "Type_Organization"]


# Display the data types of each column
df.dtypes








# Clean and convert 'Client_Income' column to numeric
df['Client_Income'] = pd.to_numeric(df['Client_Income'], errors='coerce')


# Clean and convert 'Credit_Amount' column to numeric
df['Credit_Amount'] = df['Credit_Amount'].str.replace('[^0-9.]', '', regex=True)
df['Credit_Amount'] = pd.to_numeric(df['Credit_Amount'], errors='coerce')
df['Credit_Amount'] = pd.to_numeric(df['Credit_Amount'], errors='coerce')


# Clean and convert 'Loan_Annuity' column to numeric
df['Loan_Annuity'] = df['Loan_Annuity'].str.replace('[^0-9.]', '', regex=True)
df['Loan_Annuity'] = pd.to_numeric(df['Loan_Annuity'], errors='coerce')


# Clean and convert 'Population_Region_Relative' column to numeric
df['Population_Region_Relative'] = df['Population_Region_Relative'].str.replace('[^0-9.]', '', regex=True)
df['Population_Region_Relative'] = pd.to_numeric(df['Population_Region_Relative'], errors='coerce')


# Clean and convert 'Age_Days' column to numeric
df['Age_Days'] = df['Age_Days'].str.replace('[^0-9.]', '', regex=True)
df['Age_Days'] = pd.to_numeric(df['Age_Days'], errors='coerce')



# Clean and convert 'Employed_Days' column to numeric
df['Employed_Days'] = df['Employed_Days'].str.replace('[^0-9.]', '', regex=True)
df['Employed_Days'] = pd.to_numeric(df['Employed_Days'], errors='coerce')


# Clean and convert 'Registration_Days' column to numeric
df['Registration_Days'] = df['Registration_Days'].str.replace('[^0-9.]', '', regex=True)
df['Registration_Days'] = pd.to_numeric(df['Registration_Days'], errors='coerce')


# Clean and convert 'Score_Source_3' column to numeric
df['Score_Source_3'] = df['Score_Source_3'].str.replace('[^0-9.]', '', regex=True)
df['Score_Source_3'] = pd.to_numeric(df['Score_Source_3'], errors='coerce')


# Check the the data types of each column again
df.dtypes


# Display the dimensions (number of rows and columns) of the DataFrame
df.shape


# Count the total number of duplicate rows in the DataFrame
df.duplicated().sum()


# Count the total number of missing values in each column of the DataFrame
df.isna().sum()











# Print the value counts of each unique value in the column
for i in categorical_data:
    print(df[i].value_counts())
    print('-' * 50)





# Define a mapping dictionary to combine the clusters
cluster_mapping = {"Transport: type 1":"Others", "Industry: type 10 ":"Others" ,"Industry: type 6":"Others", 
                   "Religion":"Others", "Industry: type 13":"Others", "Trade: type 4":"Others", "Trade: type 4":"Others", 
                   "Industry: type 8":"Others"}

# Update the "Client_Income_Type" column with the new cluster labels
df["Client_Income_Type"] = df["Client_Income_Type"].replace(cluster_mapping)

# Check the value_counts after replacing
df["Client_Income_Type"].value_counts()


# Define a mapping dictionary to combine the clusters
cluster_mapping = {"Student":"Govt Job", "Unemployed":"Govt Job" ,"Maternity leave":"Govt Job", "Businessman":"Govt Job"}

# Update the "Client_Income_Type" column with the new cluster labels
df["Client_Income_Type"] = df["Client_Income_Type"].replace(cluster_mapping)

# Check the value_counts after replacing
df["Client_Income_Type"].value_counts()


# Define a mapping dictionary to combine the clusters
cluster_mapping = {"Post Grad":"Junior secondary"}

# Update the "Client_Income_Type" column with the new cluster labels
df["Client_Education"] = df["Client_Education"].replace(cluster_mapping)

# Check the value_counts after replacing
df["Client_Education"].value_counts()


# Create a boolean mask to identify rows where "Client_Gender" is "XNA"
mask = df["Client_Gender"] == "XNA"

# Use the mask to drop the corresponding rows
df.drop(df[mask].index, inplace=True)

# Print the value counts of the "Client_Gender" column after removal
df['Client_Gender'].value_counts()


# Define a mapping dictionary to combine the clusters
cluster_mapping = {"##" : "Others", "Group" : "Others"}

# Update the "Accompany_Client" column with the new cluster labels
df["Accompany_Client"] = df["Accompany_Client"].replace(cluster_mapping)

# Check the value_counts after replacing
df["Accompany_Client"].value_counts()


# Label Encoding.
label_encoder = LabelEncoder()

for i in categorical_data:
    df[i] = label_encoder.fit_transform(df[i])
df.head()





# Filter the DataFrame to retain only rows where at least one element contains the string 'x'
filtered_df = df[df.applymap(lambda x: isinstance(x, str) and 'x' in x).any(axis=1)]
filtered_df


# Create a boolean mask to identify rows where "ID_Days" is equal to "x"
mask = df["ID_Days"] == "x"

# Replace the corresponding rows with NaN
df.loc[mask, "ID_Days"] = np.nan


# Initialize the IterativeImputer
imputer = IterativeImputer()

# Fit and transform the data using IterativeImputer
Iterative_imputer = imputer.fit_transform(df)

# Create a new DataFrame with imputed values
df = pd.DataFrame(Iterative_imputer, columns=df.columns)

# Check the total number of missing values in each column of the DataFrame
df.isna().sum()





# Remove the column names from the numerical_data list
numerical_data_values = numerical_data[0][1:-1]

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the StandardScaler on the selected columns to calculate mean and standard deviation
scaler.fit(df[numerical_data_values])

# Transform the selected columns using the calculated mean and standard deviation
df[numerical_data_values] = scaler.transform(df[numerical_data_values])
df.head()


# Split data into x and y.
X = df.drop("Default", axis=1)
y = df["Default"]

# Split train data into train and test.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# Initialize the LogisticRegression with modified parameters
logistic_classifier = LogisticRegression(class_weight='balanced', penalty='l2', solver='liblinear', random_state=42)

# Train the model on the scaled training data
logistic_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = logistic_classifier.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy * 100, "%")


# Create and Plot a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using seaborn's heatmap
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()








# Instantiate the SMOTE class
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Perform SMOTE oversampling on the dataset
X_overesampled, y_overesampled = smote.fit_resample(X_train, y_train)


# Train the model on the scaled training data
logistic_classifier.fit(X_overesampled, y_overesampled)

# Make predictions on the testing data
y_pred = logistic_classifier.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy * 100, "%")


# Create and Plot a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using seaborn's heatmap
# plt.figure(figsize=(15, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()


# Calculate the F1 score
f1 = f1_score(y_test, y_pred)

print("F1 score:", f1)








test = pd.read_csv("/kaggle/input/dish-network-hackathon/Test_Dataset.csv")
test.head()





test.dtypes


# Clean and convert 'Credit_Amount' column to numeric
test['Credit_Amount'] = test['Credit_Amount'].str.replace('[^0-9.]', '', regex=True)
test['Credit_Amount'] = pd.to_numeric(test['Credit_Amount'], errors='coerce')
test['Credit_Amount'] = pd.to_numeric(test['Credit_Amount'], errors='coerce')

# Clean and convert 'Loan_Annuity' column to numeric
test['Loan_Annuity'] = test['Loan_Annuity'].str.replace('[^0-9.]', '', regex=True)
test['Loan_Annuity'] = pd.to_numeric(test['Loan_Annuity'], errors='coerce')

# Clean and convert 'Population_Region_Relative' column to numeric
test['Population_Region_Relative'] = test['Population_Region_Relative'].str.replace('[^0-9.]', '', regex=True)
test['Population_Region_Relative'] = pd.to_numeric(test['Population_Region_Relative'], errors='coerce')

# Clean and convert 'Age_Days' column to numeric
test['Age_Days'] = test['Age_Days'].str.replace('[^0-9.]', '', regex=True)
test['Age_Days'] = pd.to_numeric(test['Age_Days'], errors='coerce')

# Clean and convert 'Employed_Days' column to numeric
test['Employed_Days'] = test['Employed_Days'].str.replace('[^0-9.]', '', regex=True)
test['Employed_Days'] = pd.to_numeric(test['Employed_Days'], errors='coerce')

# Clean and convert 'Registration_Days' column to numeric
test['Registration_Days'] = test['Registration_Days'].str.replace('[^0-9.]', '', regex=True)
test['Registration_Days'] = pd.to_numeric(test['Registration_Days'], errors='coerce')

# Clean and convert 'Score_Source_2' column to numeric
test['Score_Source_2'] = test['Score_Source_2'].str.replace('[^0-9.]', '', regex=True)
test['Score_Source_2'] = pd.to_numeric(test['Score_Source_2'], errors='coerce')

# Clean and convert 'Score_Source_3' column to numeric
test['Score_Source_3'] = test['Score_Source_3'].str.replace('[^0-9.]', '', regex=True)
test['Score_Source_3'] = pd.to_numeric(test['Score_Source_3'], errors='coerce')

# Check the the data types of each column again
test.dtypes


test.duplicated().sum()





test.isna().sum()


# Print the value counts of each unique value in the column
for i in categorical_data:
    print(test[i].value_counts())
    print('-' * 50)


# Define a mapping dictionary to combine the clusters
cluster_mapping = {"Transport: type 1":"Others", "Industry: type 10 ":"Others" ,"Industry: type 6":"Others", 
                   "Religion":"Others", "Industry: type 13":"Others", "Trade: type 4":"Others", "Trade: type 4":"Others", 
                   "Industry: type 8":"Others"}
# Update the "Client_Income_Type" column with the new cluster labels
test["Client_Income_Type"] = test["Client_Income_Type"].replace(cluster_mapping)

# Define a mapping dictionary to combine the clusters
cluster_mapping = {"Student":"Govt Job", "Unemployed":"Govt Job" ,"Maternity leave":"Govt Job", "Businessman":"Govt Job"}
# Update the "Client_Income_Type" column with the new cluster labels
test["Client_Income_Type"] = test["Client_Income_Type"].replace(cluster_mapping)

# Define a mapping dictionary to combine the clusters
cluster_mapping = {"Post Grad":"Junior secondary"}
# Update the "Client_Income_Type" column with the new cluster labels
test["Client_Education"] = test["Client_Education"].replace(cluster_mapping)

# Create a boolean mask to identify rows where "Client_Gender" is "XNA"
mask = test["Client_Gender"] == "XNA"
# Use the mask to drop the corresponding rows
test.drop(test[mask].index, inplace=True)

# Define a mapping dictionary to combine the clusters
cluster_mapping = {"##" : "Others", "Group" : "Others"}
# Update the "Accompany_Client" column with the new cluster labels
test["Accompany_Client"] = test["Accompany_Client"].replace(cluster_mapping)

# Label Encoding.
label_encoder = LabelEncoder()
for i in categorical_data:
    test[i] = label_encoder.fit_transform(test[i])
test.head()


# Filter the DataFrame to retain only rows where at least one element contains the string 'x'
filtered_df = test[test.applymap(lambda x: isinstance(x, str) and '$' in x).any(axis=1)]
filtered_df


# Create a boolean mask to identify rows where "Client_Income" is equal to "$"
mask = test["Client_Income"] == "$"

# Replace the corresponding rows with NaN
test.loc[mask, "Client_Income"] = np.nan


# Filter the DataFrame to retain only rows where at least one element contains the string 'x'
filtered_df = test[test.applymap(lambda x: isinstance(x, str) and 'x' in x).any(axis=1)]
filtered_df


# Create a boolean mask to identify rows where "ID_Days" is equal to "$"
mask = test["ID_Days"] == "x"

# Replace the corresponding rows with NaN
test.loc[mask, "ID_Days"] = np.nan


# Initialize the IterativeImputer
imputer = IterativeImputer()

# Fit and transform the data using IterativeImputer
Iterative_imputer = imputer.fit_transform(test)

# Create a new DataFrame with imputed values
test = pd.DataFrame(Iterative_imputer, columns=test.columns)

# Check the total number of missing values in each column of the DataFrame
test.isna().sum()


# Remshapethe column names from the numerical_data list
numerical_data_values = numerical_data[0][1:-1]

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the StandardScaler on the selected columns to calculate mean and standard deviation
scaler.fit(test[numerical_data_values])

# Transform the selected columns using the calculated mean and standard deviation
test[numerical_data_values] = scaler.transform(test[numerical_data_values])
test.head()





# Generate predictions for the test data 
test_pred = logistic_classifier.predict(test)

# Create a submission DataFrame
submission = pd.DataFrame({'ID': range(0, test.shape[0]), 'Default': test_pred})

# Save the submission DataFrame as a CSV file without including the index column.
submission.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")












