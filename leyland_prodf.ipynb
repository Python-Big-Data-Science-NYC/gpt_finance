{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21275c9e-2d91-43b4-839a-a9bc59107557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m real_data \u001b[38;5;241m=\u001b[39m fetch_financial_data(ticker, start_date, end_date)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate features\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m features \u001b[38;5;241m=\u001b[39m calculate_features(real_data)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Use sample parameters for the Leland-Toft model\u001b[39;00m\n\u001b[1;32m     55\u001b[0m sigma \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Average volatility\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     20\u001b[0m volatility \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mdropna()  \u001b[38;5;66;03m# 10-day rolling volatility\u001b[39;00m\n\u001b[1;32m     21\u001b[0m roc \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpct_change(periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()  \u001b[38;5;66;03m# Rate of change over 5 days\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m: returns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m: volatility, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc})\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    returns = data.pct_change().dropna()  # Daily returns\n",
    "    volatility = returns.rolling(window=10).std().dropna()  # 10-day rolling volatility\n",
    "    roc = data.pct_change(periods=5).dropna()  # Rate of change over 5 days\n",
    "    return pd.DataFrame({\"returns\": returns, \"volatility\": volatility, \"roc\": roc}).dropna()\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread and other calculated values\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch real data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Use sample parameters for the Leland-Toft model\n",
    "sigma = features['volatility'].mean()  # Average volatility\n",
    "r = 0.03  # Example risk-free rate\n",
    "D = 1e9   # Example total debt ($1 billion)\n",
    "E = 2e9   # Example equity value ($2 billion)\n",
    "T = 5     # Example time to maturity (5 years)\n",
    "\n",
    "# Apply Leland-Toft model\n",
    "lt_result = leland_toft(sigma, r, D, E, T)\n",
    "\n",
    "# Print results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf483b8-28b9-46f8-8047-eec444df2be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leland-Toft Model Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Series.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeland-Toft Model Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault Spread: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlt_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_spread\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCredit Spread: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlt_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredit_spread\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeverage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlt_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleverage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Series.__format__"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    returns = data.pct_change().dropna()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    \n",
    "    # Combine features into a single DataFrame with aligned indices\n",
    "    features = pd.concat(\n",
    "        {\"returns\": returns, \"volatility\": volatility, \"roc\": roc}, axis=1\n",
    "    ).dropna()  # Drop any rows with NaN values\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread, credit spread, and leverage ratio\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate financial features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Use sample parameters for the Leland-Toft model\n",
    "# Apply Leland-Toft model (if using averages for the Series)\n",
    "sigma = features['volatility'].mean()  # Average volatility\n",
    "r = 0.03  # Example risk-free rate\n",
    "D = 1e9   # Example total debt ($1 billion)\n",
    "E = 2e9   # Example equity value ($2 billion)\n",
    "T = 5     # Example time to maturity (5 years)\n",
    "\n",
    "lt_result = leland_toft(sigma, r, D, E, T)\n",
    "\n",
    "# Print results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "\n",
    "# Step 5: Visualize financial data and features (Optional)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"{ticker} Financial Features Over Time\")\n",
    "plt.plot(features.index, features['returns'], label='Returns', color='blue', alpha=0.7)\n",
    "plt.plot(features.index, features['volatility'], label='Volatility', color='green', alpha=0.7)\n",
    "plt.plot(features.index, features['roc'], label='Rate of Change', color='red', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed52637-6c01-44da-b740-54b3d9ea21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Leland-Toft Model Results:\n",
      "            (returns, AAPL)  (volatility, AAPL)  (roc, AAPL)  \\\n",
      "Date                                                           \n",
      "2022-01-18        -0.018894            0.013803    -0.013880   \n",
      "2022-01-19        -0.021025            0.014433    -0.050548   \n",
      "2022-01-20        -0.010347            0.012898    -0.062781   \n",
      "2022-01-21        -0.012765            0.012594    -0.056798   \n",
      "2022-01-24        -0.004864            0.012385    -0.066158   \n",
      "\n",
      "            (risk_free_rate, )      (debt, )    (equity, )  \\\n",
      "Date                                                         \n",
      "2022-01-18                0.03  1.000000e+09  2.000000e+09   \n",
      "2022-01-19                0.03  1.000000e+09  2.000000e+09   \n",
      "2022-01-20                0.03  1.000000e+09  2.000000e+09   \n",
      "2022-01-21                0.03  1.000000e+09  2.000000e+09   \n",
      "2022-01-24                0.03  1.000000e+09  2.000000e+09   \n",
      "\n",
      "            (time_to_maturity, )  \\\n",
      "Date                               \n",
      "2022-01-18                     5   \n",
      "2022-01-19                     5   \n",
      "2022-01-20                     5   \n",
      "2022-01-21                     5   \n",
      "2022-01-24                     5   \n",
      "\n",
      "                                               default_spread  \\\n",
      "Date                                                            \n",
      "2022-01-18  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-18 ...   \n",
      "2022-01-19  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-19 ...   \n",
      "2022-01-20  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-20 ...   \n",
      "2022-01-21  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-21 ...   \n",
      "2022-01-24  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-24 ...   \n",
      "\n",
      "                                                credit_spread  \\\n",
      "Date                                                            \n",
      "2022-01-18  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-18 ...   \n",
      "2022-01-19  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-19 ...   \n",
      "2022-01-20  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-20 ...   \n",
      "2022-01-21  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-21 ...   \n",
      "2022-01-24  Ticker\n",
      "       NaN\n",
      "AAPL   NaN\n",
      "Name: 2022-01-24 ...   \n",
      "\n",
      "                                                     leverage  \n",
      "Date                                                           \n",
      "2022-01-18  Ticker\n",
      "    0.333333\n",
      "Name: 2022-01-18 00:00:00,...  \n",
      "2022-01-19  Ticker\n",
      "    0.333333\n",
      "Name: 2022-01-19 00:00:00,...  \n",
      "2022-01-20  Ticker\n",
      "    0.333333\n",
      "Name: 2022-01-20 00:00:00,...  \n",
      "2022-01-21  Ticker\n",
      "    0.333333\n",
      "Name: 2022-01-21 00:00:00,...  \n",
      "2022-01-24  Ticker\n",
      "    0.333333\n",
      "Name: 2022-01-24 00:00:00,...  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'returns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'returns'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     90\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Financial Features Over Time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(features\u001b[38;5;241m.\u001b[39mindex, features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturns\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m     92\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(features\u001b[38;5;241m.\u001b[39mindex, features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolatility\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m     93\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(features\u001b[38;5;241m.\u001b[39mindex, features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRate of Change\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'returns'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    returns = data.pct_change().dropna()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    \n",
    "    # Combine features into a single DataFrame with aligned indices\n",
    "    features = pd.concat(\n",
    "        {\"returns\": returns, \"volatility\": volatility, \"roc\": roc}, axis=1\n",
    "    ).dropna()  # Drop any rows with NaN values\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread, credit spread, and leverage ratio\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate financial features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Add parameters as columns to features DataFrame (Example constant values)\n",
    "features['risk_free_rate'] = 0.03  # Example fixed risk-free rate\n",
    "features['debt'] = 1e9            # Example fixed total debt ($1 billion)\n",
    "features['equity'] = 2e9          # Example fixed equity value ($2 billion)\n",
    "features['time_to_maturity'] = 5  # Example fixed time to maturity (5 years)\n",
    "\n",
    "# Apply the Leland-Toft model row by row\n",
    "lt_results = features.apply(\n",
    "    lambda row: leland_toft(\n",
    "        sigma=row['volatility'],\n",
    "        r=row['risk_free_rate'],\n",
    "        D=row['debt'],\n",
    "        E=row['equity'],\n",
    "        T=row['time_to_maturity']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert results to a DataFrame for better usability\n",
    "lt_results_df = pd.DataFrame(lt_results.tolist(), index=features.index)\n",
    "\n",
    "# Add Leland-Toft results back to features\n",
    "features = pd.concat([features, lt_results_df], axis=1)\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"Sample Leland-Toft Model Results:\")\n",
    "print(features.head())\n",
    "\n",
    "# Step 5: Visualize financial data and features (Optional)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"{ticker} Financial Features Over Time\")\n",
    "plt.plot(features.index, features['returns'], label='Returns', color='blue', alpha=0.7)\n",
    "plt.plot(features.index, features['volatility'], label='Volatility', color='green', alpha=0.7)\n",
    "plt.plot(features.index, features['roc'], label='Rate of Change', color='red', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print Leland-Toft model parameters from one of the entries\n",
    "sample_lt_result = lt_results.iloc[0]  # Get first row result for printing\n",
    "print(\"\\nSample Leland-Toft Model Parameters for First Entry:\")\n",
    "print(f\"Default Spread: {sample_lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {sample_lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {sample_lt_result['leverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345e8f8b-ce62-4a1c-b8fd-3b1b1e74df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m real_data \u001b[38;5;241m=\u001b[39m fetch_financial_data(ticker, start_date, end_date)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate financial features\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m features \u001b[38;5;241m=\u001b[39m calculate_features(real_data)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Add parameters as columns to features DataFrame (Example constant values)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_free_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m  \u001b[38;5;66;03m# Example fixed risk-free rate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     26\u001b[0m roc \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpct_change(periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Combine features into a single DataFrame\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m: returns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m: volatility, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc})\n\u001b[1;32m     30\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mdropna()  \u001b[38;5;66;03m# Drop any rows with NaN values\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change().dropna()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std().dropna()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5).dropna()\n",
    "\n",
    "    # Combine features into a single DataFrame\n",
    "    features = pd.DataFrame({\"returns\": returns, \"volatility\": volatility, \"roc\": roc})\n",
    "    features = features.dropna()  # Drop any rows with NaN values\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread, credit spread, and leverage ratio\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate financial features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Add parameters as columns to features DataFrame (Example constant values)\n",
    "features['risk_free_rate'] = 0.03  # Example fixed risk-free rate\n",
    "features['debt'] = 1e9            # Example fixed total debt ($1 billion)\n",
    "features['equity'] = 2e9          # Example fixed equity value ($2 billion)\n",
    "features['time_to_maturity'] = 5  # Example fixed time to maturity (5 years)\n",
    "\n",
    "# Apply the Leland-Toft model row by row\n",
    "lt_results = features.apply(\n",
    "    lambda row: leland_toft(\n",
    "        sigma=row['volatility'],\n",
    "        r=row['risk_free_rate'],\n",
    "        D=row['debt'],\n",
    "        E=row['equity'],\n",
    "        T=row['time_to_maturity']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert results to a DataFrame for better usability\n",
    "lt_results_df = pd.DataFrame(lt_results.tolist(), index=features.index)\n",
    "\n",
    "# Add Leland-Toft results back to features\n",
    "features = pd.concat([features, lt_results_df], axis=1)\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"Sample Leland-Toft Model Results:\")\n",
    "print(features.head())\n",
    "\n",
    "# Step 5: Visualize financial data and features (Optional)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"{ticker} Financial Features Over Time\")\n",
    "plt.plot(features.index, features['returns'], label='Returns', color='blue', alpha=0.7)\n",
    "plt.plot(features.index, features['volatility'], label='Volatility', color='green', alpha=0.7)\n",
    "plt.plot(features.index, features['roc'], label='Rate of Change', color='red', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print Leland-Toft model parameters from one of the entries\n",
    "sample_lt_result = lt_results.iloc[0]  # Get first row result for printing\n",
    "print(\"\\nSample Leland-Toft Model Parameters for First Entry:\")\n",
    "print(f\"Default Spread: {sample_lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {sample_lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {sample_lt_result['leverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e07eff0-ff18-4f09-93ad-a8fac787c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m real_data \u001b[38;5;241m=\u001b[39m fetch_financial_data(ticker, start_date, end_date)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate financial features\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m features \u001b[38;5;241m=\u001b[39m calculate_features(real_data)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Add parameters as columns to features DataFrame (Example constant values)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_free_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m  \u001b[38;5;66;03m# Example fixed risk-free rate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     26\u001b[0m roc \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpct_change(periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Combine features into a single DataFrame\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m: returns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m: volatility, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc})\n\u001b[1;32m     30\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mdropna()  \u001b[38;5;66;03m# Drop any rows with NaN values\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change().dropna()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std().dropna()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5).dropna()\n",
    "\n",
    "    # Combine features into a single DataFrame\n",
    "    features = pd.DataFrame({\"returns\": returns, \"volatility\": volatility, \"roc\": roc})\n",
    "    features = features.dropna()  # Drop any rows with NaN values\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread, credit spread, and leverage ratio\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate financial features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Add parameters as columns to features DataFrame (Example constant values)\n",
    "features['risk_free_rate'] = 0.03  # Example fixed risk-free rate\n",
    "features['debt'] = 1e9            # Example fixed total debt ($1 billion)\n",
    "features['equity'] = 2e9          # Example fixed equity value ($2 billion)\n",
    "features['time_to_maturity'] = 5  # Example fixed time to maturity (5 years)\n",
    "\n",
    "# Apply the Leland-Toft model row by row\n",
    "lt_results = features.apply(\n",
    "    lambda row: leland_toft(\n",
    "        sigma=row['volatility'],\n",
    "        r=row['risk_free_rate'],\n",
    "        D=row['debt'],\n",
    "        E=row['equity'],\n",
    "        T=row['time_to_maturity']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert results to a DataFrame for better usability\n",
    "lt_results_df = pd.DataFrame(lt_results.tolist(), index=features.index)\n",
    "\n",
    "# Add Leland-Toft results back to features\n",
    "features = pd.concat([features, lt_results_df], axis=1)\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"Sample Leland-Toft Model Results:\")\n",
    "print(features.head())\n",
    "\n",
    "# Step 5: Visualize financial data and features (Optional)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"{ticker} Financial Features Over Time\")\n",
    "plt.plot(features.index, features['returns'], label='Returns', color='blue', alpha=0.7)\n",
    "plt.plot(features.index, features['volatility'], label='Volatility', color='green', alpha=0.7)\n",
    "plt.plot(features.index, features['roc'], label='Rate of Change', color='red', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print Leland-Toft model parameters from one of the entries\n",
    "sample_lt_result = lt_results.iloc[0]  # Get first row result for printing\n",
    "print(\"\\nSample Leland-Toft Model Parameters for First Entry:\")\n",
    "print(f\"Default Spread: {sample_lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {sample_lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {sample_lt_result['leverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a10b1ab8-2e6f-42f3-86ac-df8cef7d34d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m real_data \u001b[38;5;241m=\u001b[39m fetch_financial_data(ticker, start_date, end_date)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate financial features\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m features \u001b[38;5;241m=\u001b[39m calculate_features(real_data)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Add parameters as columns to features DataFrame (Example constant values)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_free_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m  \u001b[38;5;66;03m# Example fixed risk-free rate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     26\u001b[0m roc \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpct_change(periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Combine features into a single DataFrame and drop any NaN values\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m: returns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m: volatility, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc})\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Drop rows where any column contains NaN\u001b[39;00m\n\u001b[1;32m     32\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Combine features into a single DataFrame and drop any NaN values\n",
    "    features = pd.DataFrame({\"returns\": returns, \"volatility\": volatility, \"roc\": roc})\n",
    "\n",
    "    # Drop rows where any column contains NaN\n",
    "    features = features.dropna()\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread, credit spread, and leverage ratio\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate financial features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Add parameters as columns to features DataFrame (Example constant values)\n",
    "features['risk_free_rate'] = 0.03  # Example fixed risk-free rate\n",
    "features['debt'] = 1e9            # Example fixed total debt ($1 billion)\n",
    "features['equity'] = 2e9          # Example fixed equity value ($2 billion)\n",
    "features['time_to_maturity'] = 5  # Example fixed time to maturity (5 years)\n",
    "\n",
    "# Apply the Leland-Toft model row by row\n",
    "lt_results = features.apply(\n",
    "    lambda row: leland_toft(\n",
    "        sigma=row['volatility'],\n",
    "        r=row['risk_free_rate'],\n",
    "        D=row['debt'],\n",
    "        E=row['equity'],\n",
    "        T=row['time_to_maturity']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert results to a DataFrame for better usability\n",
    "lt_results_df = pd.DataFrame(lt_results.tolist(), index=features.index)\n",
    "\n",
    "# Add Leland-Toft results back to features\n",
    "features = pd.concat([features, lt_results_df], axis=1)\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"Sample Leland-Toft Model Results:\")\n",
    "print(features.head())\n",
    "\n",
    "# Step 5: Visualize financial data and features (Optional)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"{ticker} Financial Features Over Time\")\n",
    "plt.plot(features.index, features['returns'], label='Returns', color='blue', alpha=0.7)\n",
    "plt.plot(features.index, features['volatility'], label='Volatility', color='green', alpha=0.7)\n",
    "plt.plot(features.index, features['roc'], label='Rate of Change', color='red', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print Leland-Toft model parameters from one of the entries\n",
    "sample_lt_result = lt_results.iloc[0]  # Get first row result for printing\n",
    "print(\"\\nSample Leland-Toft Model Parameters for First Entry:\")\n",
    "print(f\"Default Spread: {sample_lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {sample_lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {sample_lt_result['leverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d92584bd-24d5-409b-b03a-d32f3365f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m real_data \u001b[38;5;241m=\u001b[39m fetch_financial_data(ticker, start_date, end_date)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Calculate financial features\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m features \u001b[38;5;241m=\u001b[39m calculate_features(real_data)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Add parameters as columns to features DataFrame (Example constant values)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_free_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m  \u001b[38;5;66;03m# Example fixed risk-free rate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 35\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     32\u001b[0m roc \u001b[38;5;241m=\u001b[39m roc\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Ensure all features have the same index (aligned)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m: returns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m: volatility, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m\"\u001b[39m: roc})\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Drop any rows with NaN in any column (further precaution)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Drop NaN values before combining the features\n",
    "    # Align the features by the common index\n",
    "    returns = returns.dropna()\n",
    "    volatility = volatility.dropna()\n",
    "    roc = roc.dropna()\n",
    "\n",
    "    # Ensure all features have the same index (aligned)\n",
    "    features = pd.DataFrame({\"returns\": returns, \"volatility\": volatility, \"roc\": roc})\n",
    "\n",
    "    # Drop any rows with NaN in any column (further precaution)\n",
    "    features = features.dropna()\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Define the Leland-Toft model\n",
    "def leland_toft(sigma, r, D, E, T, bankruptcy_cost=0.25, tax_rate=0.35):\n",
    "    \"\"\"\n",
    "    Calculate default spread using Leland-Toft model.\n",
    "    :param sigma: Volatility\n",
    "    :param r: Risk-free rate\n",
    "    :param D: Debt\n",
    "    :param E: Equity\n",
    "    :param T: Time to maturity\n",
    "    :param bankruptcy_cost: Cost of bankruptcy (default = 25%)\n",
    "    :param tax_rate: Corporate tax rate (default = 35%)\n",
    "    :return: Default spread, credit spread, and leverage ratio\n",
    "    \"\"\"\n",
    "    asset_value = E + D\n",
    "    default_spread = (sigma**2 * (1 - bankruptcy_cost)) / (2 * T)\n",
    "    leverage = D / asset_value\n",
    "    credit_spread = default_spread + leverage * tax_rate * r\n",
    "    return {\"default_spread\": default_spread, \"credit_spread\": credit_spread, \"leverage\": leverage}\n",
    "\n",
    "# Step 4: Example calculation using financial data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate financial features\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Add parameters as columns to features DataFrame (Example constant values)\n",
    "features['risk_free_rate'] = 0.03  # Example fixed risk-free rate\n",
    "features['debt'] = 1e9            # Example fixed total debt ($1 billion)\n",
    "features['equity'] = 2e9          # Example fixed equity value ($2 billion)\n",
    "features['time_to_maturity'] = 5  # Example fixed time to maturity (5 years)\n",
    "\n",
    "# Apply the Leland-Toft model row by row\n",
    "lt_results = features.apply(\n",
    "    lambda row: leland_toft(\n",
    "        sigma=row['volatility'],\n",
    "        r=row['risk_free_rate'],\n",
    "        D=row['debt'],\n",
    "        E=row['equity'],\n",
    "        T=row['time_to_maturity']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert results to a DataFrame for better usability\n",
    "lt_results_df = pd.DataFrame(lt_results.tolist(), index=features.index)\n",
    "\n",
    "# Add Leland-Toft results back to features\n",
    "features = pd.concat([features, lt_results_df], axis=1)\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"Sample Leland-Toft Model Results:\")\n",
    "print(features.head())\n",
    "\n",
    "# Step 5: Visualize financial data and features (Optional)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"{ticker} Financial Features Over Time\")\n",
    "plt.plot(features.index, features['returns'], label='Returns', color='blue', alpha=0.7)\n",
    "plt.plot(features.index, features['volatility'], label='Volatility', color='green', alpha=0.7)\n",
    "plt.plot(features.index, features['roc'], label='Rate of Change', color='red', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print Leland-Toft model parameters from one of the entries\n",
    "sample_lt_result = lt_results.iloc[0]  # Get first row result for printing\n",
    "print(\"\\nSample Leland-Toft Model Parameters for First Entry:\")\n",
    "print(f\"Default Spread: {sample_lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {sample_lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {sample_lt_result['leverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aade8ae-e028-4d6c-a1c7-b4099f679a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Adj Close       Close        High         Low        Open  \\\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL        AAPL   \n",
      "Date                                                                     \n",
      "2022-01-03  179.076630  182.009995  182.880005  177.710007  177.830002   \n",
      "2022-01-04  176.803802  179.699997  182.940002  179.119995  182.630005   \n",
      "2022-01-05  172.100861  174.919998  180.169998  174.639999  179.610001   \n",
      "2022-01-06  169.227905  172.000000  175.300003  171.639999  172.699997   \n",
      "2022-01-07  169.395172  172.169998  174.139999  171.029999  172.889999   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2022-12-23  130.487823  131.860001  132.419998  129.639999  130.919998   \n",
      "2022-12-27  128.676834  130.029999  131.410004  128.720001  131.380005   \n",
      "2022-12-28  124.728371  126.040001  131.029999  125.870003  129.669998   \n",
      "2022-12-29  128.261215  129.610001  130.479996  127.730003  127.989998   \n",
      "2022-12-30  128.577881  129.929993  129.949997  127.430000  128.410004   \n",
      "\n",
      "Price          Volume  \n",
      "Ticker           AAPL  \n",
      "Date                   \n",
      "2022-01-03  104487900  \n",
      "2022-01-04   99310400  \n",
      "2022-01-05   94537600  \n",
      "2022-01-06   96904000  \n",
      "2022-01-07   86709100  \n",
      "...               ...  \n",
      "2022-12-23   63814900  \n",
      "2022-12-27   69007800  \n",
      "2022-12-28   85438400  \n",
      "2022-12-29   75703700  \n",
      "2022-12-30   77034200  \n",
      "\n",
      "[251 rows x 6 columns]\n",
      "Error fetching data: real_data is not a pandas Series.\n",
      "Simulating stock prices due to data fetch error.\n",
      "             returns  volatility       roc\n",
      "2022-01-17 -0.210616    0.385677 -0.347845\n",
      "2022-01-18  0.529042    0.415058  0.884328\n",
      "2022-01-19 -0.411024    0.438764 -0.159638\n",
      "2022-01-20  0.181748    0.427242 -0.305082\n",
      "2022-01-21  0.356200    0.433216  0.139340\n",
      "Leland-Toft Model Results:\n",
      "Default Spread: 0.280914\n",
      "Credit Spread: 0.272486\n",
      "Leverage: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    print(stock_data)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "try:\n",
    "    real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "    if isinstance(real_data, pd.Series):\n",
    "        real_data = real_data['Close']  # Assuming the 'Close' column if a DataFrame is returned\n",
    "    if not isinstance(real_data, pd.Series):\n",
    "        raise ValueError(\"real_data is not a pandas Series.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    # Simulate data in case of failure\n",
    "    print(\"Simulating stock prices due to data fetch error.\")\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"B\")  # Business days\n",
    "    prices = np.random.uniform(100, 200, size=len(dates))  # Simulated stock prices\n",
    "    real_data = pd.Series(data=prices, index=dates, name=\"Close\")\n",
    "\n",
    "# Step 5: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Print the features DataFrame for verification\n",
    "print(features.head())\n",
    "\n",
    "# Step 6: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 7: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "994d53af-7f69-48df-aab4-69efa00f5544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 22:12:16.203806: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737861136.226877  800806 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737861136.233508  800806 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-25 22:12:16.257171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "2025-01-25 22:12:18.250709: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Adj Close       Close        High         Low        Open  \\\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL        AAPL   \n",
      "Date                                                                     \n",
      "2022-01-03  179.076630  182.009995  182.880005  177.710007  177.830002   \n",
      "2022-01-04  176.803802  179.699997  182.940002  179.119995  182.630005   \n",
      "2022-01-05  172.100861  174.919998  180.169998  174.639999  179.610001   \n",
      "2022-01-06  169.227905  172.000000  175.300003  171.639999  172.699997   \n",
      "2022-01-07  169.395172  172.169998  174.139999  171.029999  172.889999   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2022-12-23  130.487823  131.860001  132.419998  129.639999  130.919998   \n",
      "2022-12-27  128.676834  130.029999  131.410004  128.720001  131.380005   \n",
      "2022-12-28  124.728371  126.040001  131.029999  125.870003  129.669998   \n",
      "2022-12-29  128.261215  129.610001  130.479996  127.730003  127.989998   \n",
      "2022-12-30  128.577881  129.929993  129.949997  127.430000  128.410004   \n",
      "\n",
      "Price          Volume  \n",
      "Ticker           AAPL  \n",
      "Date                   \n",
      "2022-01-03  104487900  \n",
      "2022-01-04   99310400  \n",
      "2022-01-05   94537600  \n",
      "2022-01-06   96904000  \n",
      "2022-01-07   86709100  \n",
      "...               ...  \n",
      "2022-12-23   63814900  \n",
      "2022-12-27   69007800  \n",
      "2022-12-28   85438400  \n",
      "2022-12-29   75703700  \n",
      "2022-12-30   77034200  \n",
      "\n",
      "[251 rows x 6 columns]\n",
      "Error fetching data: real_data is not a pandas Series.\n",
      "Simulating stock prices due to data fetch error.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    141\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 142\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    145\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 102\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# VAE loss function\u001b[39;00m\n\u001b[1;32m    101\u001b[0m xent_loss \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean)\n\u001b[0;32m--> 102\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m vae_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(xent_loss \u001b[38;5;241m+\u001b[39m kl_loss)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:2080\u001b[0m, in \u001b[0;36msquare\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.square\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare\u001b[39m(x):\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msquare(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py:12067\u001b[0m, in \u001b[0;36msquare\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m  12065\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m  12066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m> 12067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m square_eager_fallback(\n\u001b[1;32m  12068\u001b[0m       x, name\u001b[38;5;241m=\u001b[39mname, ctx\u001b[38;5;241m=\u001b[39m_ctx)\n\u001b[1;32m  12069\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m  12070\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py:12108\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[0;34m(x, name, ctx)\u001b[0m\n\u001b[1;32m  12107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x: Annotated[Any, TV_Square_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Square_T]:\n\u001b[0;32m> 12108\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39margs_to_matching_eager([x], ctx, [_dtypes\u001b[38;5;241m.\u001b[39mbfloat16, _dtypes\u001b[38;5;241m.\u001b[39mhalf, _dtypes\u001b[38;5;241m.\u001b[39mfloat32, _dtypes\u001b[38;5;241m.\u001b[39mfloat64, _dtypes\u001b[38;5;241m.\u001b[39mint8, _dtypes\u001b[38;5;241m.\u001b[39mint16, _dtypes\u001b[38;5;241m.\u001b[39mint32, _dtypes\u001b[38;5;241m.\u001b[39mint64, _dtypes\u001b[38;5;241m.\u001b[39muint8, _dtypes\u001b[38;5;241m.\u001b[39muint16, _dtypes\u001b[38;5;241m.\u001b[39muint32, _dtypes\u001b[38;5;241m.\u001b[39muint64, _dtypes\u001b[38;5;241m.\u001b[39mcomplex64, _dtypes\u001b[38;5;241m.\u001b[39mcomplex128, ])\n\u001b[1;32m  12109\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m  12110\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m overload(dtype, name)  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    print(stock_data)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function\n",
    "    xent_loss = input_shape[0] * tf.keras.losses.binary_crossentropy(inputs, x_decoded_mean)\n",
    "    kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    # Compile model\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "try:\n",
    "    real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "    if isinstance(real_data, pd.Series):\n",
    "        real_data = real_data['Close']  # Assuming the 'Close' column if a DataFrame is returned\n",
    "    if not isinstance(real_data, pd.Series):\n",
    "        raise ValueError(\"real_data is not a pandas Series.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    # Simulate data in case of failure\n",
    "    print(\"Simulating stock prices due to data fetch error.\")\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"B\")  # Business days\n",
    "    prices = np.random.uniform(100, 200, size=len(dates))  # Simulated stock prices\n",
    "    real_data = pd.Series(data=prices, index=dates, name=\"Close\")\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1501564-7f2d-43e6-b71c-7dbac6716101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Adj Close       Close        High         Low        Open  \\\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL        AAPL   \n",
      "Date                                                                     \n",
      "2022-01-03  179.076630  182.009995  182.880005  177.710007  177.830002   \n",
      "2022-01-04  176.803802  179.699997  182.940002  179.119995  182.630005   \n",
      "2022-01-05  172.100861  174.919998  180.169998  174.639999  179.610001   \n",
      "2022-01-06  169.227905  172.000000  175.300003  171.639999  172.699997   \n",
      "2022-01-07  169.395172  172.169998  174.139999  171.029999  172.889999   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2022-12-23  130.487823  131.860001  132.419998  129.639999  130.919998   \n",
      "2022-12-27  128.676834  130.029999  131.410004  128.720001  131.380005   \n",
      "2022-12-28  124.728371  126.040001  131.029999  125.870003  129.669998   \n",
      "2022-12-29  128.261215  129.610001  130.479996  127.730003  127.989998   \n",
      "2022-12-30  128.577881  129.929993  129.949997  127.430000  128.410004   \n",
      "\n",
      "Price          Volume  \n",
      "Ticker           AAPL  \n",
      "Date                   \n",
      "2022-01-03  104487900  \n",
      "2022-01-04   99310400  \n",
      "2022-01-05   94537600  \n",
      "2022-01-06   96904000  \n",
      "2022-01-07   86709100  \n",
      "...               ...  \n",
      "2022-12-23   63814900  \n",
      "2022-12-27   69007800  \n",
      "2022-12-28   85438400  \n",
      "2022-12-29   75703700  \n",
      "2022-12-30   77034200  \n",
      "\n",
      "[251 rows x 6 columns]\n",
      "Error fetching data: real_data is not a pandas Series.\n",
      "Simulating stock prices due to data fetch error.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    147\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 148\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    151\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 107\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m K\u001b[38;5;241m.\u001b[39mmean(xent_loss \u001b[38;5;241m+\u001b[39m kl_loss)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Apply VAE loss\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m vae_loss_val \u001b[38;5;241m=\u001b[39m vae_loss(inputs, x_decoded_mean, z_mean, z_log_var)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Add loss to model\u001b[39;00m\n\u001b[1;32m    110\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(vae_loss_val)\n",
      "Cell \u001b[0;32mIn[22], line 103\u001b[0m, in \u001b[0;36mbuild_vae.<locals>.vae_loss\u001b[0;34m(inputs, x_decoded_mean, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvae_loss\u001b[39m(inputs, x_decoded_mean, z_mean, z_log_var):\n\u001b[1;32m    102\u001b[0m     xent_loss \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean)\n\u001b[0;32m--> 103\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m K\u001b[38;5;241m.\u001b[39mmean(xent_loss \u001b[38;5;241m+\u001b[39m kl_loss)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:2080\u001b[0m, in \u001b[0;36msquare\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.square\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare\u001b[39m(x):\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msquare(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py:12067\u001b[0m, in \u001b[0;36msquare\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m  12065\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m  12066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m> 12067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m square_eager_fallback(\n\u001b[1;32m  12068\u001b[0m       x, name\u001b[38;5;241m=\u001b[39mname, ctx\u001b[38;5;241m=\u001b[39m_ctx)\n\u001b[1;32m  12069\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m  12070\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py:12108\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[0;34m(x, name, ctx)\u001b[0m\n\u001b[1;32m  12107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x: Annotated[Any, TV_Square_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Square_T]:\n\u001b[0;32m> 12108\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39margs_to_matching_eager([x], ctx, [_dtypes\u001b[38;5;241m.\u001b[39mbfloat16, _dtypes\u001b[38;5;241m.\u001b[39mhalf, _dtypes\u001b[38;5;241m.\u001b[39mfloat32, _dtypes\u001b[38;5;241m.\u001b[39mfloat64, _dtypes\u001b[38;5;241m.\u001b[39mint8, _dtypes\u001b[38;5;241m.\u001b[39mint16, _dtypes\u001b[38;5;241m.\u001b[39mint32, _dtypes\u001b[38;5;241m.\u001b[39mint64, _dtypes\u001b[38;5;241m.\u001b[39muint8, _dtypes\u001b[38;5;241m.\u001b[39muint16, _dtypes\u001b[38;5;241m.\u001b[39muint32, _dtypes\u001b[38;5;241m.\u001b[39muint64, _dtypes\u001b[38;5;241m.\u001b[39mcomplex64, _dtypes\u001b[38;5;241m.\u001b[39mcomplex128, ])\n\u001b[1;32m  12109\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m  12110\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m overload(dtype, name)  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    print(stock_data)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        xent_loss = input_shape[0] * tf.keras.losses.binary_crossentropy(inputs, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    \n",
    "    # Apply VAE loss\n",
    "    vae_loss_val = vae_loss(inputs, x_decoded_mean, z_mean, z_log_var)\n",
    "    \n",
    "    # Add loss to model\n",
    "    vae.add_loss(vae_loss_val)\n",
    "    \n",
    "    # Compile model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "try:\n",
    "    real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "    if isinstance(real_data, pd.Series):\n",
    "        real_data = real_data['Close']  # Assuming the 'Close' column if a DataFrame is returned\n",
    "    if not isinstance(real_data, pd.Series):\n",
    "        raise ValueError(\"real_data is not a pandas Series.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    # Simulate data in case of failure\n",
    "    print(\"Simulating stock prices due to data fetch error.\")\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"B\")  # Business days\n",
    "    prices = np.random.uniform(100, 200, size=len(dates))  # Simulated stock prices\n",
    "    real_data = pd.Series(data=prices, index=dates, name=\"Close\")\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b77d427a-347b-40a5-bb52-466d423e30f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Adj Close       Close        High         Low        Open  \\\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL        AAPL   \n",
      "Date                                                                     \n",
      "2022-01-03  179.076630  182.009995  182.880005  177.710007  177.830002   \n",
      "2022-01-04  176.803802  179.699997  182.940002  179.119995  182.630005   \n",
      "2022-01-05  172.100861  174.919998  180.169998  174.639999  179.610001   \n",
      "2022-01-06  169.227905  172.000000  175.300003  171.639999  172.699997   \n",
      "2022-01-07  169.395172  172.169998  174.139999  171.029999  172.889999   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2022-12-23  130.487823  131.860001  132.419998  129.639999  130.919998   \n",
      "2022-12-27  128.676834  130.029999  131.410004  128.720001  131.380005   \n",
      "2022-12-28  124.728371  126.040001  131.029999  125.870003  129.669998   \n",
      "2022-12-29  128.261215  129.610001  130.479996  127.730003  127.989998   \n",
      "2022-12-30  128.577881  129.929993  129.949997  127.430000  128.410004   \n",
      "\n",
      "Price          Volume  \n",
      "Ticker           AAPL  \n",
      "Date                   \n",
      "2022-01-03  104487900  \n",
      "2022-01-04   99310400  \n",
      "2022-01-05   94537600  \n",
      "2022-01-06   96904000  \n",
      "2022-01-07   86709100  \n",
      "...               ...  \n",
      "2022-12-23   63814900  \n",
      "2022-12-27   69007800  \n",
      "2022-12-28   85438400  \n",
      "2022-12-29   75703700  \n",
      "2022-12-30   77034200  \n",
      "\n",
      "[251 rows x 6 columns]\n",
      "Error fetching data: real_data is not a pandas Series.\n",
      "Simulating stock prices due to data fetch error.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    145\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 146\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    149\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 107\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xent_loss \u001b[38;5;241m+\u001b[39m kl_loss\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Compute loss and add it to the model\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(inputs, x_decoded_mean, z_mean, z_log_var)\n\u001b[1;32m    108\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(loss)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 102\u001b[0m, in \u001b[0;36mbuild_vae.<locals>.vae_loss\u001b[0;34m(inputs, x_decoded_mean, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvae_loss\u001b[39m(inputs, x_decoded_mean, z_mean, z_log_var):\n\u001b[0;32m--> 102\u001b[0m     xent_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(K\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xent_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:277\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.binary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinary_crossentropy\u001b[39m(target, output, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m    278\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    print(stock_data)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (using Keras backend to ensure it's linked correctly within the model)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    \n",
    "    # Compute loss and add it to the model\n",
    "    loss = vae_loss(inputs, x_decoded_mean, z_mean, z_log_var)\n",
    "    vae.add_loss(loss)\n",
    "    \n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "try:\n",
    "    real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "    if isinstance(real_data, pd.Series):\n",
    "        real_data = real_data['Close']  # Assuming the 'Close' column if a DataFrame is returned\n",
    "    if not isinstance(real_data, pd.Series):\n",
    "        raise ValueError(\"real_data is not a pandas Series.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    # Simulate data in case of failure\n",
    "    print(\"Simulating stock prices due to data fetch error.\")\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"B\")  # Business days\n",
    "    prices = np.random.uniform(100, 200, size=len(dates))  # Simulated stock prices\n",
    "    real_data = pd.Series(data=prices, index=dates, name=\"Close\")\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8626a3ae-4058-45d0-b7de-7b1b66915b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 144\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    143\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 144\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    147\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 118\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    115\u001b[0m vae_loss_layer \u001b[38;5;241m=\u001b[39m VaeLossLayer()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Compute and add loss to model\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(vae_loss_layer(inputs, x_decoded_mean, z_mean, z_log_var))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m    121\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:331\u001b[0m, in \u001b[0;36mFunctional.add_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Symbolic only. TODO\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (using a custom layer for calculation)\n",
    "    class VaeLossLayer(layers.Layer):\n",
    "        def __init__(self):\n",
    "            super(VaeLossLayer, self).__init__()\n",
    "\n",
    "        def call(self, inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "            # Compute cross entropy loss\n",
    "            xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "            \n",
    "            # Compute KL divergence\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "            # Return the total loss\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "    # Instantiate the custom layer for loss computation\n",
    "    vae_loss_layer = VaeLossLayer()\n",
    "\n",
    "    # Compute and add loss to model\n",
    "    vae.add_loss(vae_loss_layer(inputs, x_decoded_mean, z_mean, z_log_var))\n",
    "    \n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fc14d22-f5aa-48cc-a0dc-b678aeae0451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    139\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 140\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    143\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 111\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xent_loss \u001b[38;5;241m+\u001b[39m kl_loss\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Define the VAE loss (This is computed inside the model's `call` method)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(inputs, x_decoded_mean, z_mean, z_log_var)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Add the loss to the VAE model\u001b[39;00m\n\u001b[1;32m    114\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(loss)\n",
      "Cell \u001b[0;32mIn[34], line 102\u001b[0m, in \u001b[0;36mbuild_vae.<locals>.vae_loss\u001b[0;34m(inputs, x_decoded_mean, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvae_loss\u001b[39m(inputs, x_decoded_mean, z_mean, z_log_var):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Compute reconstruction loss (binary crossentropy)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     xent_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(K\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Compute KL divergence loss\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:277\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.binary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinary_crossentropy\u001b[39m(target, output, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m    278\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (calculated in the call method)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        # Compute reconstruction loss (binary crossentropy)\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Combine both losses\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    # Define the VAE loss (This is computed inside the model's `call` method)\n",
    "    loss = vae_loss(inputs, x_decoded_mean, z_mean, z_log_var)\n",
    "\n",
    "    # Add the loss to the VAE model\n",
    "    vae.add_loss(loss)\n",
    "\n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3f9189b-0845-4ba4-bd1d-f7377c3bdd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 112\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    115\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 88\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Add custom loss layer\u001b[39;00m\n\u001b[1;32m     87\u001b[0m vae_loss_layer \u001b[38;5;241m=\u001b[39m VaeLossLayer()\n\u001b[0;32m---> 88\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(vae_loss_layer([x_decoded_mean, inputs, z_mean, z_log_var]))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     91\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:331\u001b[0m, in \u001b[0;36mFunctional.add_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Symbolic only. TODO\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model\n",
    "def leland_toft_model(features):\n",
    "    risk_free_rate = 0.03\n",
    "    asset_price = features['returns'].mean() * 100\n",
    "    volatility = features['volatility'].mean()\n",
    "    sigma = volatility\n",
    "    leverage = asset_price / (asset_price + sigma)\n",
    "    default_spread = sigma * leverage\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)\n",
    "\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile VAE model\n",
    "class VaeLossLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VaeLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_decoded_mean, inputs, z_mean, z_log_var = inputs\n",
    "\n",
    "        # Reconstruction loss\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "\n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "        # Total loss\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # Add custom loss layer\n",
    "    vae_loss_layer = VaeLossLayer()\n",
    "    vae.add_loss(vae_loss_layer([x_decoded_mean, inputs, z_mean, z_log_var]))\n",
    "\n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d296a8-3196-45d2-ae16-f59764a5a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Build VAE model\u001b[39;00m\n\u001b[1;32m    136\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 137\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    140\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 111\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xent_loss \u001b[38;5;241m+\u001b[39m kl_loss\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Define the VAE loss as a Lambda layer instead\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(\u001b[38;5;28;01mlambda\u001b[39;00m: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m    114\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:331\u001b[0m, in \u001b[0;36mFunctional.add_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Symbolic only. TODO\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (calculated in the call method)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        # Compute reconstruction loss (binary crossentropy)\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Combine both losses\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    # Define the VAE loss as a Lambda layer instead\n",
    "    vae.add_loss(lambda: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n",
    "\n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "encoded_data_np = K.function([vae.input], [encoded_data])([features_values])[0]  # Evaluation step\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data_np[:, 0], encoded_data_np[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70bdeec6-6254-40aa-9c92-6c387af6620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.91860911e-01 -1.17235723e+00 -1.93217270e-01]\n",
      " [-8.85793616e-01 -1.07981239e+00 -9.92028548e-01]\n",
      " [-4.15057172e-01 -1.30532441e+00 -1.25852074e+00]\n",
      " [-5.21657773e-01 -1.35000602e+00 -1.12816941e+00]\n",
      " [-1.73340477e-01 -1.38062339e+00 -1.33208689e+00]\n",
      " [-4.60800483e-01 -1.39851815e+00 -1.17637712e+00]\n",
      " [ 1.62753226e-02 -1.82724693e+00 -7.47923365e-01]\n",
      " [-8.86477703e-02 -1.91897738e+00 -5.91356700e-01]\n",
      " [ 3.11733237e+00  6.30338786e-01  1.17149967e+00]\n",
      " [ 1.19288884e+00  8.27247601e-01  1.88299305e+00]\n",
      " [-1.77305502e-03  6.95585316e-01  2.13111155e+00]\n",
      " [ 3.51660223e-01  4.91665915e-01  2.31232319e+00]\n",
      " [-6.96002139e-01  5.69974618e-01  1.98087672e+00]\n",
      " [-8.89316925e-02  4.83981283e-01  3.72623917e-01]\n",
      " [-1.45578339e-01  4.79538620e-01 -2.79723976e-01]\n",
      " [ 8.55232843e-01  3.95549875e-01  1.36603445e-01]\n",
      " [ 4.06746159e-01  3.60656095e-01  1.63667336e-01]\n",
      " [-9.99274032e-01  6.59940201e-01  1.08785691e-02]\n",
      " [-8.50244798e-01 -7.97693984e-01 -3.64728507e-01]\n",
      " [ 1.03849459e-01 -1.22778589e+00 -2.43644292e-01]\n",
      " [ 1.06180883e+00 -8.71926696e-01 -1.45040568e-01]\n",
      " [-2.01248625e-02 -9.08541495e-01 -3.51799093e-01]\n",
      " [-8.96568639e-01 -8.29060415e-01 -3.00921319e-01]\n",
      " [-3.71352375e-01 -8.05856262e-01 -6.39439848e-02]\n",
      " [-7.44167435e-01 -7.06348461e-01 -4.79063760e-01]\n",
      " [-1.09914340e+00 -8.26368822e-01 -1.49453640e+00]\n",
      " [ 7.76471464e-01 -6.56835293e-01 -1.12937531e+00]\n",
      " [ 6.12703915e-01 -6.33496597e-01 -4.10696284e-01]\n",
      " [ 1.13310486e-01 -7.63659001e-01 -1.74711521e-01]\n",
      " [-4.71521735e-01 -7.32186913e-01 -3.93300976e-02]\n",
      " [ 9.48762122e-01 -7.95997079e-01  9.92412354e-01]\n",
      " [-4.62399807e-02 -7.97113248e-01  5.76334002e-01]\n",
      " [-7.70439578e-01 -8.44322461e-01 -1.12855838e-01]\n",
      " [-1.00450813e+00 -6.68208949e-01 -6.58693104e-01]\n",
      " [-4.73646255e-01 -7.28757983e-01 -6.59717830e-01]\n",
      " [ 1.58400691e+00 -4.14375275e-01 -3.63004749e-01]\n",
      " [-1.15742711e+00 -2.31431855e-01 -9.01253916e-01]\n",
      " [-1.01293487e+00 -1.96293465e-01 -1.01766606e+00]\n",
      " [-1.12992596e+00 -8.74486333e-02 -1.07786228e+00]\n",
      " [ 1.34946526e+00  3.83030568e-01 -2.16011698e-01]\n",
      " [ 1.32028328e+00  5.44028482e-01 -3.40042536e-01]\n",
      " [ 3.25639960e-01  5.75166581e-01  3.97748854e-01]\n",
      " [ 9.63341578e-01  6.35132857e-01  1.41148281e+00]\n",
      " [ 4.17500039e-01  4.27847060e-01  2.24395423e+00]\n",
      " [ 9.58124036e-01  4.04417326e-01  2.03774599e+00]\n",
      " [ 4.04095000e-01  1.13609309e-01  1.55883568e+00]\n",
      " [ 1.04088584e+00 -2.42653308e-01  1.93337037e+00]\n",
      " [ 2.05728572e-01 -7.59575412e-01  1.53596686e+00]\n",
      " [ 2.63153508e-01 -1.71176190e+00  1.45539049e+00]\n",
      " [ 8.84667972e-01 -1.88150265e+00  1.41763506e+00]\n",
      " [-2.52045589e-01 -1.78829152e+00  1.07674133e+00]\n",
      " [-7.42558756e-01 -1.25714169e+00  1.76735426e-01]\n",
      " [-3.46388592e-02 -1.32154129e+00  5.80347161e-02]\n",
      " [ 1.08565867e+00 -1.15208174e+00  4.61482751e-01]\n",
      " [-7.93970669e-01 -9.39771909e-01 -3.65591787e-01]\n",
      " [-7.72316648e-01 -7.36202410e-01 -6.18760076e-01]\n",
      " [ 1.20642384e-01 -9.98115916e-01 -1.99007772e-01]\n",
      " [-4.83911380e-01 -9.59423022e-01 -4.18248158e-01]\n",
      " [-1.08378891e+00 -7.67152674e-01 -1.44009761e+00]\n",
      " [ 5.49128713e-01 -9.32851500e-01 -8.11712375e-01]\n",
      " [ 7.61585761e-01 -6.99538127e-01 -7.21421669e-02]\n",
      " [-1.28095895e+00 -4.84284752e-01 -7.57730599e-01]\n",
      " [-1.75677067e-02 -4.82995732e-01 -5.33793998e-01]\n",
      " [ 6.63388481e-01 -7.00067160e-01  3.26016687e-01]\n",
      " [-3.66346251e-03 -7.81140012e-01  5.32828288e-02]\n",
      " [-1.72429176e-01 -8.89917995e-01 -3.99666507e-01]\n",
      " [-1.18542160e+00 -6.43433975e-01 -3.52134811e-01]\n",
      " [ 3.38123794e-01 -5.99642916e-01 -1.79865397e-01]\n",
      " [-1.60454375e+00 -3.24035066e-01 -1.27028490e+00]\n",
      " [-2.35590223e-02 -4.46148831e-01 -1.27950295e+00]\n",
      " [ 2.03183551e+00  3.42908132e-01 -2.54752772e-01]\n",
      " [-1.57265501e+00  4.71243390e-01 -4.48288802e-01]\n",
      " [ 1.27800562e-01  4.81119138e-01 -5.48881078e-01]\n",
      " [ 4.65330671e-01  4.33405726e-01  4.81496207e-01]\n",
      " [ 1.84900440e+00  1.01017819e+00  1.42400494e+00]\n",
      " [-2.41520221e+00  1.73993212e+00 -8.05420579e-01]\n",
      " [ 1.84525079e-01  1.61171490e+00  5.80280153e-02]\n",
      " [-1.42207667e+00  1.78544806e+00 -7.04533449e-01]\n",
      " [ 7.51423990e-01  1.60355139e+00 -5.69740342e-01]\n",
      " [-2.24437270e+00  2.15393529e+00 -2.45221489e+00]\n",
      " [-1.14455421e+00  1.51019913e+00 -1.86546757e+00]\n",
      " [ 1.44817692e+00  1.76345705e+00 -1.29948531e+00]\n",
      " [-4.29394799e-01  1.74722399e+00 -8.24928172e-01]\n",
      " [ 1.16189241e+00  1.93041509e+00 -6.33873912e-01]\n",
      " [-2.44619469e+00  1.78140328e+00 -7.35467741e-01]\n",
      " [-1.04523611e+00  1.37728304e+00 -6.86989563e-01]\n",
      " [ 1.18138077e-01  1.36522481e+00 -1.30061311e+00]\n",
      " [ 1.80980723e+00  1.82846617e+00 -2.54571424e-01]\n",
      " [-8.06051109e-01  1.73103433e+00 -1.18707082e+00]\n",
      " [ 9.13629300e-02  1.21604134e+00  6.27452774e-02]\n",
      " [ 1.06388263e+00  1.19963417e+00  1.12900181e+00]\n",
      " [ 1.83791166e+00  1.36340311e+00  2.01704501e+00]\n",
      " [-1.94585550e-01  1.33415971e+00  9.81398017e-01]\n",
      " [ 2.60422901e-03  1.17982101e+00  1.40512971e+00]\n",
      " [ 7.82249054e-01  1.05793819e-01  1.76642285e+00]\n",
      " [-1.65866195e+00  4.71693496e-01  3.51579817e-01]\n",
      " [ 2.71573216e-01  4.65633588e-01 -4.00378540e-01]\n",
      " [ 8.16401858e-01  9.87387638e-02  9.01298036e-02]\n",
      " [-1.81235759e-01 -8.02861820e-02 -7.13137440e-04]\n",
      " [-1.54404029e+00  4.70835694e-01 -1.12552204e+00]\n",
      " [-1.66188094e+00  7.20869627e-01 -1.12708269e+00]\n",
      " [-1.64672239e+00  2.26244383e-01 -2.01655028e+00]\n",
      " [ 3.35278192e-01  3.28715611e-01 -2.22738721e+00]\n",
      " [ 9.27743058e-01  5.98835132e-01 -1.73569046e+00]\n",
      " [-1.70697085e+00  5.84236451e-01 -1.81213161e+00]\n",
      " [ 5.49558027e-01  5.40725142e-01 -7.75707739e-01]\n",
      " [ 1.48539845e+00  9.94206504e-01  7.68247175e-01]\n",
      " [-1.27615343e-01  8.08483414e-01  5.34154054e-01]\n",
      " [ 9.92206503e-01  1.05502433e+00  5.65989295e-01]\n",
      " [ 1.12197661e+00  9.73390723e-01  2.05213133e+00]\n",
      " [ 4.11068897e-02  4.79994728e-01  1.78159610e+00]\n",
      " [-1.27220245e+00  2.63324766e-01  3.60883271e-01]\n",
      " [ 6.15276171e-01  2.85848915e-01  7.33645135e-01]\n",
      " [-7.53663479e-01  3.39885822e-01 -1.35051089e-01]\n",
      " [ 7.53730565e-01 -3.16012717e-01 -3.10671138e-01]\n",
      " [ 8.75675887e-01 -2.66819371e-01  9.37764165e-02]\n",
      " [ 4.64652576e-01 -5.61879487e-01  9.77755857e-01]\n",
      " [ 1.09915113e+00 -4.75366492e-01  1.22319541e+00]\n",
      " [ 2.48956868e-01 -5.65669641e-01  1.75352811e+00]\n",
      " [-6.09510398e-01 -5.84660167e-01  1.04057118e+00]\n",
      " [ 3.42380342e-01 -5.80349873e-01  7.70886019e-01]\n",
      " [-7.07239246e-02 -1.15757282e+00  5.00892266e-01]\n",
      " [ 9.44099480e-01 -1.06691921e+00  4.24725062e-01]\n",
      " [ 5.45897346e-01 -1.48172793e+00  5.72882631e-01]\n",
      " [-8.68971792e-01 -1.03264855e+00  4.39981499e-01]\n",
      " [ 1.21917210e+00 -8.93794755e-01  8.76834744e-01]\n",
      " [ 6.36705254e-01 -8.74500019e-01  1.23964367e+00]\n",
      " [ 7.06550344e-01 -9.97210822e-01  1.11864770e+00]\n",
      " [-3.16465801e-01 -8.99663046e-01  6.77819378e-01]\n",
      " [-2.85053997e-01 -1.02786899e+00  9.80129887e-01]\n",
      " [-3.48012665e-01 -9.30607393e-01  1.95718427e-01]\n",
      " [ 1.55038326e+00 -5.69511929e-01  6.42956382e-01]\n",
      " [ 1.98570909e-01 -6.51243882e-01  3.89616012e-01]\n",
      " [ 1.48682515e+00 -3.64396185e-01  1.29954882e+00]\n",
      " [-2.30175517e-01 -6.51031962e-01  1.32836152e+00]\n",
      " [-3.68336216e-01 -6.84235294e-01  1.31766233e+00]\n",
      " [ 1.72729784e+00 -2.76509459e-01  1.40687850e+00]\n",
      " [-4.38138065e-02 -2.62843195e-01  1.28042299e+00]\n",
      " [-8.11971373e-02 -3.17461480e-01  4.89864387e-01]\n",
      " [-8.68750440e-02 -3.65044647e-01  5.62358650e-01]\n",
      " [ 5.44776651e-02 -4.62567495e-01  7.77634322e-01]\n",
      " [ 1.19592309e+00 -6.22158341e-01  5.16972541e-01]\n",
      " [-1.54264038e-01 -5.64482544e-01  4.61265492e-01]\n",
      " [ 9.85678387e-01 -7.87662789e-01  9.98464106e-01]\n",
      " [ 3.20326463e-01 -8.68479186e-01  1.20850306e+00]\n",
      " [ 3.77414938e-04 -9.79848946e-01  1.18042934e+00]\n",
      " [ 4.28386925e-01 -1.61169362e+00  7.92664499e-01]\n",
      " [-5.99234414e-02 -1.60778305e+00  8.40958761e-01]\n",
      " [-6.24675459e-01 -1.37767172e+00  3.57377572e-02]\n",
      " [-9.74168098e-01 -1.00096610e+00 -5.97757536e-01]\n",
      " [-4.83469829e-02 -9.94688334e-01 -6.21074530e-01]\n",
      " [ 1.20195364e-01 -1.39495826e+00 -7.66978216e-01]\n",
      " [ 6.98990594e-01 -1.26531321e+00 -4.06222759e-01]\n",
      " [-1.62090617e+00 -8.56550356e-01 -8.94225094e-01]\n",
      " [-5.62440610e-01 -9.03033822e-01 -6.95569339e-01]\n",
      " [-6.33652830e-01 -8.96002282e-01 -9.74675621e-01]\n",
      " [-4.27747299e-01 -1.07271269e+00 -1.23150516e+00]\n",
      " [ 2.48612027e-01 -9.86416645e-01 -1.43728895e+00]\n",
      " [-5.58953378e-01 -9.94218982e-01 -9.30686620e-01]\n",
      " [-3.21066002e-01 -1.10834768e+00 -8.15530141e-01]\n",
      " [ 4.49076066e-01 -9.70339979e-01 -2.95255697e-01]\n",
      " [-3.82906645e-01 -1.01378602e+00 -2.73275652e-01]\n",
      " [ 8.71678179e-01 -9.10258338e-01  2.77851866e-02]\n",
      " [ 1.73877278e+00 -5.97254128e-01  1.17455455e+00]\n",
      " [-2.54584520e+00  5.54837629e-01  1.18829136e-02]\n",
      " [ 4.62366809e-01  5.60506703e-01  1.83611453e-02]\n",
      " [-7.93438541e-01  6.25396142e-01 -1.85615982e-01]\n",
      " [-4.42083454e-01  6.20723749e-01 -8.14173897e-01]\n",
      " [ 1.14691589e+00  8.14552270e-01 -1.08385430e+00]\n",
      " [ 7.31735386e-01  8.57772076e-01  5.42471562e-01]\n",
      " [-8.52414387e-01  9.59435493e-01 -1.13867422e-01]\n",
      " [-2.39950824e-01  9.46187068e-01  1.62056984e-01]\n",
      " [-6.25643523e-01  8.60631343e-01  7.01245046e-02]\n",
      " [ 1.40753149e-01  2.51071719e-01 -4.14027373e-01]\n",
      " [ 3.30586293e-01 -9.25901831e-01 -6.04507221e-01]\n",
      " [-5.16650352e-01 -9.45058924e-01 -4.40707824e-01]\n",
      " [-2.12436127e+00 -1.52354382e-01 -1.35419332e+00]\n",
      " [-1.28320860e+00  3.76139082e-02 -1.66195135e+00]\n",
      " [ 1.39686622e+00  1.85498624e-01 -1.09300277e+00]\n",
      " [ 1.17072908e+00  3.77332778e-01 -7.03321573e-01]\n",
      " [ 1.31629080e-01  3.26329186e-01 -3.90975589e-01]\n",
      " [-2.50994032e-01  3.26649729e-01  5.60201296e-01]\n",
      " [-1.57767629e+00  6.11783517e-01  4.07080279e-01]\n",
      " [ 1.44957949e-01  6.12328867e-01 -2.01290562e-01]\n",
      " [-4.10994634e-01  5.50729644e-01 -9.52499172e-01]\n",
      " [-1.61908505e-01  5.47806933e-01 -1.09019670e+00]\n",
      " [ 1.52296886e+00  3.16755714e-01 -2.56343673e-01]\n",
      " [-1.38022993e+00  3.63908773e-01 -1.56757534e-01]\n",
      " [ 1.32501304e+00  3.31285033e-01  4.17885125e-01]\n",
      " [ 4.55931792e-01  1.26177613e-01  8.56841790e-01]\n",
      " [ 7.48425292e-02  1.23519130e-01  9.78405285e-01]\n",
      " [-1.02925655e-01  1.14795547e-01  1.70095299e-01]\n",
      " [ 1.23403731e+00 -2.17140535e-01  1.50868261e+00]\n",
      " [ 6.93699934e-01 -1.89649155e-01  1.18607857e+00]\n",
      " [ 8.93624868e-01 -2.70350788e-01  1.41093687e+00]\n",
      " [-8.24174842e-01 -2.53650442e-02  9.40508868e-01]\n",
      " [-1.30199249e+00  1.22159285e-01  3.23372734e-01]\n",
      " [ 3.37192221e+00  1.12123000e+00  1.36207192e+00]\n",
      " [-6.38275708e-01  1.19923356e+00  6.76186205e-01]\n",
      " [-7.32283785e-01  1.35131334e+00 -1.32516419e-01]\n",
      " [-1.60352431e+00  1.76346930e+00 -5.20977326e-01]\n",
      " [-1.82836331e+00  2.16656134e+00 -7.81492054e-01]\n",
      " [-1.17613232e-01  1.94474922e+00 -2.31914510e+00]\n",
      " [ 2.13141855e-01  1.86508893e+00 -1.93947037e+00]\n",
      " [ 2.25169807e-01  1.72486952e+00 -1.50319109e+00]\n",
      " [-1.42211081e+00  1.83879618e+00 -1.41696530e+00]\n",
      " [ 3.96364896e+00  3.45089561e+00  1.36246824e+00]\n",
      " [ 8.90591160e-01  2.39528813e+00  1.89122937e+00]\n",
      " [-3.77077386e-01  2.37129962e+00  1.57694587e+00]\n",
      " [ 5.64382441e-01  2.35118744e+00  1.75511464e+00]\n",
      " [-3.26179371e-01  2.03587466e+00  2.35757196e+00]\n",
      " [ 6.12963408e-01  1.48451205e+00  6.80214909e-01]\n",
      " [ 2.07831468e-01  1.45064385e+00  3.40536319e-01]\n",
      " [-9.14689056e-01  1.66968807e+00  6.94874997e-02]\n",
      " [ 6.87460279e-01  1.68030122e+00  1.29482501e-01]\n",
      " [ 3.02375227e-01  1.20633506e+00  4.42978906e-01]\n",
      " [-8.22698981e-01 -1.03814769e+00 -2.68088688e-01]\n",
      " [-1.11678192e+00 -9.31712827e-01 -9.08876911e-01]\n",
      " [-8.91237764e-01 -7.98569825e-01 -8.97585390e-01]\n",
      " [ 2.18342249e+00  2.07064612e-01 -2.02717639e-01]\n",
      " [ 1.24495763e-01  1.88149330e-01 -2.88846459e-01]\n",
      " [-1.07521332e-01  1.21552651e-01  6.50295975e-02]\n",
      " [-3.10840062e-01  1.19787353e-01  4.73192216e-01]\n",
      " [-1.07735693e+00  1.73815175e-01  3.77665965e-01]\n",
      " [-5.66615967e-01  6.85681655e-02 -9.34240477e-01]\n",
      " [ 5.75993062e-01  1.35384718e-01 -7.22223711e-01]\n",
      " [-1.10325011e-01  5.50720705e-02 -7.23560760e-01]\n",
      " [ 7.63677914e-01 -4.69162229e-02 -2.08783369e-01]\n",
      " [ 3.40118520e-01 -2.43538891e-01  4.99393804e-01]\n",
      " [-6.43806484e-01 -1.28900395e+00  4.60024810e-01]\n",
      " [-2.02451723e+00 -4.46482183e-01 -8.30039576e-01]\n",
      " [-6.01615352e-01 -4.43442032e-01 -1.06314085e+00]\n",
      " [-6.60285098e-01 -4.27452737e-01 -1.71817910e+00]\n",
      " [ 1.77958866e-02 -5.18624007e-01 -1.86310904e+00]\n",
      " [ 1.09077394e+00 -1.81396415e-01 -1.07127871e+00]\n",
      " [-1.00693524e+00 -1.74222250e-01 -5.72317302e-01]\n",
      " [-8.22512224e-02 -1.72096608e-01 -3.20028146e-01]\n",
      " [-5.70736337e-01 -4.23839932e-01 -2.75948886e-01]\n",
      " [-1.31168444e+00 -4.31889441e-01 -9.21628122e-01]\n",
      " [ 1.28981659e+00  1.91918679e-01 -8.30105857e-01]\n",
      " [ 1.49950366e-01 -3.77800412e-01 -2.69767761e-01]]\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(features_values)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Step 7: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/optree/ops.py:752\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    750\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    751\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treespec\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;28mmap\u001b[39m(func, \u001b[38;5;241m*\u001b[39mflat_args))\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # Define the VAE loss function\n",
    "    def vae_loss(y_true, y_pred):\n",
    "        # Reconstruction loss\n",
    "        xent_loss = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Return total VAE loss (reconstruction + KL)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    # Compile the model with custom loss\n",
    "    vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Ensure there are no NaN or None values before normalizing\n",
    "if np.any(np.isnan(features_values)):\n",
    "    print(\"Warning: NaN values found in features\")\n",
    "else:\n",
    "    # Normalize data for VAE input\n",
    "    features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "\n",
    "features_values = np.nan_to_num(features_values)\n",
    "\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Now proceed to model training\n",
    "print(features_values)\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "encoded_data_np = K.function([vae.input], [encoded_data])([features_values])[0]  # Evaluation step\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data_np[:, 0], encoded_data_np[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60ee42f1-d18d-4f3a-95fe-4bca51b552a4",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN or None values\n",
    "    features = features.dropna()\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (calculated in the call method)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        # Compute reconstruction loss (binary crossentropy)\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Combine both losses\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    # Define the VAE loss as a Lambda layer instead\n",
    "    vae.add_loss(lambda: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n",
    "\n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Ensure there are no NaN or None values before normalizing\n",
    "if np.any(np.isnan(features_values)):\n",
    "    print(\"NaN values detected in features_values\")\n",
    "    # Remove NaN values or fill with 0 or mean value\n",
    "    features_values = np.nan_to_num(features_values)\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 8: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 9: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "829bea4d-326e-436a-8b9b-0d5de7ef39f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for VAE: (241, 3)\n",
      "Data preview: [[-0.79186091 -1.17235723 -0.19321727]\n",
      " [-0.88579362 -1.07981239 -0.99202855]\n",
      " [-0.41505717 -1.30532441 -1.25852074]\n",
      " [-0.52165777 -1.35000602 -1.12816941]\n",
      " [-0.17334048 -1.38062339 -1.33208689]]\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Step 7: Train the VAE model\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Step 8: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    141\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/optree/ops.py:752\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    750\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    751\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treespec\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;28mmap\u001b[39m(func, \u001b[38;5;241m*\u001b[39mflat_args))\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (calculated in the compile method)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        total_loss = xent_loss + kl_loss\n",
    "        return total_loss\n",
    "\n",
    "    # Compile the model\n",
    "    vae.compile(optimizer='rmsprop', loss=lambda y_true, y_pred: vae_loss(inputs, y_pred, z_mean, z_log_var))\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Debugging outputs before model fitting\n",
    "print(\"Data shape for VAE:\", features_values.shape)\n",
    "print(\"Data preview:\", features_values[:5])  # Just first few rows for review\n",
    "\n",
    "# Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Step 7: Train the VAE model\n",
    "vae.fit(features_values, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 8: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 9: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 10: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "350d4af2-4624-475d-a109-76305c213007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (241, 3)\n",
      "Input data sample:\n",
      " [[-0.01889411  0.01380321 -0.01388001]\n",
      " [-0.02102478  0.01443331 -0.05054836]\n",
      " [-0.01034712  0.0128979  -0.06278132]\n",
      " [-0.01276513  0.01259368 -0.05679772]\n",
      " [-0.00486429  0.01238522 -0.06615827]]\n",
      "Checking for NaNs in features_values: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Step 7: Build VAE model\u001b[39;00m\n\u001b[1;32m    141\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 142\u001b[0m vae, _, _ \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    145\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 111\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    108\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m xent_loss \u001b[38;5;241m+\u001b[39m kl_loss\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[0;32m--> 111\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(\u001b[38;5;28;01mlambda\u001b[39;00m: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n\u001b[1;32m    112\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vae, z_mean, z_log_var\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:331\u001b[0m, in \u001b[0;36mFunctional.add_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Symbolic only. TODO\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (calculated in the call method)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        # Compute reconstruction loss (binary crossentropy)\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Combine both losses\n",
    "        total_loss = xent_loss + kl_loss\n",
    "        return total_loss\n",
    "\n",
    "    vae.add_loss(lambda: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    \n",
    "    return vae, z_mean, z_log_var\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Debug: Check shape of features_values before training\n",
    "print(\"Input data shape:\", features_values.shape)\n",
    "print(\"Input data sample:\\n\", features_values[:5])  # Print a few samples to verify the data\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Check for NaNs in the features_values after normalization\n",
    "print(\"Checking for NaNs in features_values:\", np.isnan(features_values).any())\n",
    "\n",
    "# Step 7: Build VAE model\n",
    "latent_dim = 2\n",
    "vae, _, _ = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 8: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 9: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 10: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11f106e4-d3ae-41f8-b3c4-0a3d956c142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (241, 3)\n",
      "Input data sample:\n",
      " [[-0.01889411  0.01380321 -0.01388001]\n",
      " [-0.02102478  0.01443331 -0.05054836]\n",
      " [-0.01034712  0.0128979  -0.06278132]\n",
      " [-0.01276513  0.01259368 -0.05679772]\n",
      " [-0.00486429  0.01238522 -0.06615827]]\n",
      "Checking for NaNs in features_values: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Step 7: Build VAE model\u001b[39;00m\n\u001b[1;32m    144\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 145\u001b[0m vae \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[1;32m    148\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 112\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Calculate loss and add it to model (done within custom training loop)\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Compile model (rmsprop is used here but can be customized)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 102\u001b[0m, in \u001b[0;36mbuild_vae.<locals>.vae_loss\u001b[0;34m(inputs, x_decoded_mean, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvae_loss\u001b[39m(inputs, x_decoded_mean, z_mean, z_log_var):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Compute reconstruction loss (binary crossentropy)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     xent_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(K\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Compute KL divergence loss\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:277\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.binary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinary_crossentropy\u001b[39m(target, output, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m    278\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # VAE loss function (calculated using custom training loop)\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        # Compute reconstruction loss (binary crossentropy)\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Combine both losses\n",
    "        total_loss = xent_loss + kl_loss\n",
    "        return total_loss\n",
    "\n",
    "    # Calculate loss and add it to model (done within custom training loop)\n",
    "    vae.add_loss(vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n",
    "\n",
    "    # Compile model (rmsprop is used here but can be customized)\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "\n",
    "    return vae\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Debug: Check shape of features_values before training\n",
    "print(\"Input data shape:\", features_values.shape)\n",
    "print(\"Input data sample:\\n\", features_values[:5])  # Print a few samples to verify the data\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Check for NaNs in the features_values after normalization\n",
    "print(\"Checking for NaNs in features_values:\", np.isnan(features_values).any())\n",
    "\n",
    "# Step 7: Build VAE model\n",
    "latent_dim = 2\n",
    "vae = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 8: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 9: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 10: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "533974b7-d9b6-4d52-afb9-d9a13497f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (241, 3)\n",
      "Input data sample:\n",
      " [[-0.01889411  0.01380321 -0.01388001]\n",
      " [-0.02102478  0.01443331 -0.05054836]\n",
      " [-0.01034712  0.0128979  -0.06278132]\n",
      " [-0.01276513  0.01259368 -0.05679772]\n",
      " [-0.00486429  0.01238522 -0.06615827]]\n",
      "Checking for NaNs in features_values: False\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 175\u001b[0m\n\u001b[1;32m    172\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Step 8: Train the VAE using a custom loop\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m train_vae(vae, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Step 9: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    178\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "Cell \u001b[0;32mIn[62], line 134\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(vae, features_values, batch_size, epochs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     x_decoded_mean \u001b[38;5;241m=\u001b[39m vae(batch)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Compute the VAE loss\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     loss \u001b[38;5;241m=\u001b[39m vae_loss(batch, x_decoded_mean, z_mean, z_log_var)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Compute gradients and apply the optimizer\u001b[39;00m\n\u001b[1;32m    137\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, vae\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[62], line 105\u001b[0m, in \u001b[0;36mvae_loss\u001b[0;34m(inputs, x_decoded_mean, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom loss function to combine reconstruction and KL loss.\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Compute reconstruction loss (binary crossentropy)\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m xent_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(K\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Compute KL divergence loss\u001b[39;00m\n\u001b[1;32m    108\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:289\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    286\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(output, epsilon_, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon_)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Compute cross entropy from probabilities.\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m bce \u001b[38;5;241m=\u001b[39m target \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(output \u001b[38;5;241m+\u001b[39m backend\u001b[38;5;241m.\u001b[39mepsilon())\n\u001b[1;32m    290\u001b[0m bce \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m output \u001b[38;5;241m+\u001b[39m backend\u001b[38;5;241m.\u001b[39mepsilon())\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mbce\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model (without loss calculation in the model)\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    return vae, z_mean, z_log_var, inputs, x_decoded_mean\n",
    "\n",
    "# VAE loss function\n",
    "def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "    \"\"\"Custom loss function to combine reconstruction and KL loss.\"\"\"\n",
    "    # Compute reconstruction loss (binary crossentropy)\n",
    "    xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "    \n",
    "    # Compute KL divergence loss\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    \n",
    "    # Total loss (Reconstruction loss + KL divergence loss)\n",
    "    total_loss = xent_loss + kl_loss\n",
    "    return total_loss\n",
    "\n",
    "# Custom training loop for VAE\n",
    "def train_vae(vae, features_values, batch_size=32, epochs=50):\n",
    "    \"\"\"Custom training loop to apply VAE loss manually.\"\"\"\n",
    "    # Prepare data\n",
    "    n_samples = features_values.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        # Shuffle the data\n",
    "        np.random.shuffle(features_values)\n",
    "        \n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch = features_values[start:end]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                x_decoded_mean = vae(batch)\n",
    "                # Compute the VAE loss\n",
    "                loss = vae_loss(batch, x_decoded_mean, z_mean, z_log_var)\n",
    "                \n",
    "            # Compute gradients and apply the optimizer\n",
    "            gradients = tape.gradient(loss, vae.trainable_variables)\n",
    "            vae.optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
    "            total_loss += loss\n",
    "        \n",
    "        print(f\"Loss for epoch {epoch+1}: {total_loss.numpy()}\")\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Debug: Check shape of features_values before training\n",
    "print(\"Input data shape:\", features_values.shape)\n",
    "print(\"Input data sample:\\n\", features_values[:5])  # Print a few samples to verify the data\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Check for NaNs in the features_values after normalization\n",
    "print(\"Checking for NaNs in features_values:\", np.isnan(features_values).any())\n",
    "\n",
    "# Step 7: Build VAE model\n",
    "latent_dim = 2\n",
    "vae, z_mean, z_log_var, inputs, x_decoded_mean = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Compile model (without loss as it's calculated manually)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "\n",
    "# Step 8: Train the VAE using a custom loop\n",
    "train_vae(vae, features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 9: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 10: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 11: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2aed4312-f6f1-450d-9fd4-bc1737c1583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (241, 3)\n",
      "Input data sample:\n",
      " [[-0.01889411  0.01380321 -0.01388001]\n",
      " [-0.02102478  0.01443331 -0.05054836]\n",
      " [-0.01034712  0.0128979  -0.06278132]\n",
      " [-0.01276513  0.01259368 -0.05679772]\n",
      " [-0.00486429  0.01238522 -0.06615827]]\n",
      "Checking for NaNs in features_values: False\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 182\u001b[0m\n\u001b[1;32m    179\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Step 8: Train the VAE using a custom loop\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m train_vae(vae, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Step 9: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    185\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "Cell \u001b[0;32mIn[66], line 140\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(vae, features_values, batch_size, epochs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     x_decoded_mean \u001b[38;5;241m=\u001b[39m vae(batch)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Compute the VAE loss\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     loss \u001b[38;5;241m=\u001b[39m vae_loss(batch, x_decoded_mean, z_mean, z_log_var)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Compute gradients and apply the optimizer\u001b[39;00m\n\u001b[1;32m    143\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, vae\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[66], line 112\u001b[0m, in \u001b[0;36mvae_loss\u001b[0;34m(inputs, x_decoded_mean, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    109\u001b[0m xent_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(K\u001b[38;5;241m.\u001b[39mbinary_crossentropy(inputs, x_decoded_mean), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Compute KL divergence loss\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Total loss (Reconstruction loss + KL divergence loss)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m xent_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/legacy/backend.py:2080\u001b[0m, in \u001b[0;36msquare\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.square\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare\u001b[39m(x):\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msquare(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py:12067\u001b[0m, in \u001b[0;36msquare\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m  12065\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m  12066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m> 12067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m square_eager_fallback(\n\u001b[1;32m  12068\u001b[0m       x, name\u001b[38;5;241m=\u001b[39mname, ctx\u001b[38;5;241m=\u001b[39m_ctx)\n\u001b[1;32m  12069\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m  12070\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py:12108\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[0;34m(x, name, ctx)\u001b[0m\n\u001b[1;32m  12107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x: Annotated[Any, TV_Square_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Square_T]:\n\u001b[0;32m> 12108\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39margs_to_matching_eager([x], ctx, [_dtypes\u001b[38;5;241m.\u001b[39mbfloat16, _dtypes\u001b[38;5;241m.\u001b[39mhalf, _dtypes\u001b[38;5;241m.\u001b[39mfloat32, _dtypes\u001b[38;5;241m.\u001b[39mfloat64, _dtypes\u001b[38;5;241m.\u001b[39mint8, _dtypes\u001b[38;5;241m.\u001b[39mint16, _dtypes\u001b[38;5;241m.\u001b[39mint32, _dtypes\u001b[38;5;241m.\u001b[39mint64, _dtypes\u001b[38;5;241m.\u001b[39muint8, _dtypes\u001b[38;5;241m.\u001b[39muint16, _dtypes\u001b[38;5;241m.\u001b[39muint32, _dtypes\u001b[38;5;241m.\u001b[39muint64, _dtypes\u001b[38;5;241m.\u001b[39mcomplex64, _dtypes\u001b[38;5;241m.\u001b[39mcomplex128, ])\n\u001b[1;32m  12109\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m  12110\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m overload(dtype, name)  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model (without loss calculation in the model)\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    return vae, z_mean, z_log_var, inputs, x_decoded_mean\n",
    "\n",
    "# VAE loss function\n",
    "def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "    \"\"\"Custom loss function to combine reconstruction and KL loss.\"\"\"\n",
    "    # Ensure inputs and decoded mean are of the same type\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "    x_decoded_mean = tf.cast(x_decoded_mean, tf.float32)\n",
    "    \n",
    "    # Compute reconstruction loss (binary crossentropy)\n",
    "    xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "    \n",
    "    # Compute KL divergence loss\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    \n",
    "    # Total loss (Reconstruction loss + KL divergence loss)\n",
    "    total_loss = xent_loss + kl_loss\n",
    "    return total_loss\n",
    "\n",
    "# Custom training loop for VAE\n",
    "def train_vae(vae, features_values, batch_size=32, epochs=50):\n",
    "    \"\"\"Custom training loop to apply VAE loss manually.\"\"\"\n",
    "    # Prepare data\n",
    "    n_samples = features_values.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        # Shuffle the data\n",
    "        np.random.shuffle(features_values)\n",
    "        \n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch = features_values[start:end]\n",
    "            \n",
    "            batch = tf.cast(batch, tf.float32)  # Make sure the batch is cast to float32\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                x_decoded_mean = vae(batch)\n",
    "                # Compute the VAE loss\n",
    "                loss = vae_loss(batch, x_decoded_mean, z_mean, z_log_var)\n",
    "                \n",
    "            # Compute gradients and apply the optimizer\n",
    "            gradients = tape.gradient(loss, vae.trainable_variables)\n",
    "            vae.optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
    "            total_loss += loss\n",
    "        \n",
    "        print(f\"Loss for epoch {epoch+1}: {total_loss.numpy()}\")\n",
    "\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Debug: Check shape of features_values before training\n",
    "print(\"Input data shape:\", features_values.shape)\n",
    "print(\"Input data sample:\\n\", features_values[:5])  # Print a few samples to verify the data\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Check for NaNs in the features_values after normalization\n",
    "print(\"Checking for NaNs in features_values:\", np.isnan(features_values).any())\n",
    "\n",
    "# Step 7: Build VAE model\n",
    "latent_dim = 2\n",
    "vae, z_mean, z_log_var, inputs, x_decoded_mean = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Compile model (without loss as it's calculated manually)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "\n",
    "# Step 8: Train the VAE using a custom loop\n",
    "train_vae(vae, features_values, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 9: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 10: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 11: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "392c59de-7a91-47dd-ba8c-f8103ea3cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Step 7: Build VAE model\u001b[39;00m\n\u001b[1;32m    139\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 140\u001b[0m vae, z_mean, z_log_var, inputs, x_decoded_mean \u001b[38;5;241m=\u001b[39m build_vae(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Compile the model (no need for a specific loss, as it's already handled in Lambda layer)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 117\u001b[0m, in \u001b[0;36mbuild_vae\u001b[0;34m(latent_dim, input_shape)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Lambda layer that applies custom loss to the final output\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m vae\u001b[38;5;241m.\u001b[39madd_loss(\u001b[38;5;28;01mlambda\u001b[39;00m: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vae, z_mean, z_log_var, inputs, x_decoded_mean\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:331\u001b[0m, in \u001b[0;36mFunctional.add_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Symbolic only. TODO\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    # Constants for the Leland-Toft model (Example, these should be adjusted based on your real data and assumptions)\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    # Calculate some parameters using features\n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    # Default spread based on Leland-Toft model assumptions\n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption (could be more complex)\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "\n",
    "    # Calculate credit spread\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # This is an example calculation\n",
    "    \n",
    "    # Package results into a dictionary for easy access\n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the VAE model with loss in Lambda Layer\n",
    "def build_vae(latent_dim, input_shape):\n",
    "    \"\"\"Build a Variational Autoencoder model.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    # Sampling Layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hid = layers.Dense(32, activation='relu')\n",
    "    decoder_out = layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    h_decoded = decoder_hid(z)\n",
    "    x_decoded_mean = decoder_out(h_decoded)\n",
    "\n",
    "    # VAE model (no loss here, handled in Lambda layer)\n",
    "    vae = models.Model(inputs, x_decoded_mean)\n",
    "\n",
    "    # Define custom loss in Lambda layer\n",
    "    def vae_loss(inputs, x_decoded_mean, z_mean, z_log_var):\n",
    "        \"\"\"Custom loss function to combine reconstruction and KL loss.\"\"\"\n",
    "        # Ensure consistent data type\n",
    "        inputs = K.cast(inputs, dtype='float32')\n",
    "        x_decoded_mean = K.cast(x_decoded_mean, dtype='float32')\n",
    "\n",
    "        # Reconstruction loss (binary crossentropy)\n",
    "        xent_loss = K.mean(K.binary_crossentropy(inputs, x_decoded_mean), axis=-1)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        \n",
    "        # Total loss (combining both)\n",
    "        total_loss = xent_loss + kl_loss\n",
    "        return total_loss\n",
    "    \n",
    "    # Lambda layer that applies custom loss to the final output\n",
    "    vae.add_loss(lambda: vae_loss(inputs, x_decoded_mean, z_mean, z_log_var))\n",
    "\n",
    "    return vae, z_mean, z_log_var, inputs, x_decoded_mean\n",
    "\n",
    "# Step 5: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 6: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the VAE\n",
    "features_values = features.values\n",
    "\n",
    "# Normalize data for VAE input\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "# Step 7: Build VAE model\n",
    "latent_dim = 2\n",
    "vae, z_mean, z_log_var, inputs, x_decoded_mean = build_vae(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Compile the model (no need for a specific loss, as it's already handled in Lambda layer)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "\n",
    "# Step 8: Train the VAE\n",
    "vae.fit(features_values, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 9: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 10: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 11: Visualize Latent Space (VAE Compression)\n",
    "encoded_data = vae.layers[2].output  # z_mean of the VAE encoding\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=features['returns'], cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"VAE Latent Space Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9dcb6c26-8301-4a5b-8c87-3ca8b5b9fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Real Data Shape: (32, 3), Fake Data Shape: (32, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m    141\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m--> 142\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs, batch_size)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Step 10: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    145\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "Cell \u001b[0;32mIn[72], line 109\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Data Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreal_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fake Data Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfake_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Train the discriminator (real data and fake data)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_data, real_labels, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    110\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, fake_labels, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train generator through GAN (flip labels, wants to fool discriminator)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m one_step_on_data(data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(step_function, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # Credit spread formula\n",
    "    \n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the GAN model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator: Classifies if data is real or fake\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # Output: 1 (real/fake)\n",
    "    ])\n",
    "\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator: Creates synthetic financial data\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # GAN: Stack generator and discriminator\n",
    "    discriminator.trainable = False  # We train the generator, discriminator stays frozen in GAN training\n",
    "    \n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Step 5: Train the GAN model\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size):\n",
    "    batch_count = features_values.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            # Train discriminator with a mix of real and fake data\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "            fake_data = generator.predict(np.random.randn(batch_size, latent_dim))\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))  # Real = 1\n",
    "            fake_labels = np.zeros((batch_size, 1))  # Fake = 0\n",
    "            print(f\"Real Data Shape: {real_data.shape}, Fake Data Shape: {fake_data.shape}\")\n",
    "\n",
    "            # Train the discriminator (real data and fake data)\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, real_labels, return_dict=True)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels, return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "            # Train generator through GAN (flip labels, wants to fool discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            g_loss = gan.train_on_batch(noise, real_labels)  # Generator wants the discriminator to classify as \"real\"\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - D Loss Real: {d_loss_real[0]:.4f}, D Loss Fake: {d_loss_fake[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Step 6: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 7: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the GAN\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)  # Normalize data\n",
    "\n",
    "# Step 8: Build GAN model\n",
    "latent_dim = 10  # Latent space dimension (random noise)\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Step 9: Train the GAN\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "train_gan(generator, discriminator, gan, features_values, epochs, batch_size)\n",
    "\n",
    "# Step 10: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 11: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 12: Visualize Generator Output\n",
    "noise = np.random.randn(1000, latent_dim)\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(generated_data[:, 0], generated_data[:, 1], color='red', alpha=0.5, label=\"Generated Data\")\n",
    "plt.scatter(features['returns'], features['volatility'], color='blue', alpha=0.5, label=\"Real Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Generated vs Real Financial Data\")\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9a59ca0-405f-4bb9-9741-a8d8fb5c9314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Epoch 1/10000\n",
      "Real Data Shape: (32, 3)\n",
      "Fake Data Shape: (32, 3)\n",
      "Real Labels Shape: (32, 1)\n",
      "Fake Labels Shape: (32, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m    145\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m--> 146\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Step 10: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    149\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "Cell \u001b[0;32mIn[74], line 114\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFake Labels Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfake_labels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Train the discriminator (real data and fake data)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_data, real_labels)\n\u001b[1;32m    115\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, fake_labels)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Train generator through GAN (flip labels, wants to fool discriminator)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m one_step_on_data(data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(step_function, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # Credit spread formula\n",
    "    \n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the GAN model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator: Classifies if data is real or fake\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # Output: 1 (real/fake)\n",
    "    ])\n",
    "    \n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator: Creates synthetic financial data\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # GAN: Stack generator and discriminator\n",
    "    discriminator.trainable = False  # We train the generator, discriminator stays frozen in GAN training\n",
    "    \n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Step 5: Train the GAN model with debugging\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    batch_count = features_values.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            # Train discriminator with a mix of real and fake data\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "            fake_data = generator.predict(np.random.randn(batch_size, latent_dim))\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))  # Real = 1\n",
    "            fake_labels = np.zeros((batch_size, 1))  # Fake = 0\n",
    "\n",
    "            # Print the shapes of data and labels for debugging\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            print(f\"Real Data Shape: {real_data.shape}\")\n",
    "            print(f\"Fake Data Shape: {fake_data.shape}\")\n",
    "            print(f\"Real Labels Shape: {real_labels.shape}\")\n",
    "            print(f\"Fake Labels Shape: {fake_labels.shape}\")\n",
    "\n",
    "            # Train the discriminator (real data and fake data)\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "            # Train generator through GAN (flip labels, wants to fool discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            g_loss = gan.train_on_batch(noise, real_labels)  # Generator wants the discriminator to classify as \"real\"\n",
    "\n",
    "            # Log the losses\n",
    "            print(f\"D Loss Real: {d_loss_real[0]:.4f}, D Loss Fake: {d_loss_fake[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Step 6: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 7: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the GAN\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)  # Normalize data\n",
    "\n",
    "# Step 8: Build GAN model\n",
    "latent_dim = 10  # Latent space dimension (random noise)\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Step 9: Train the GAN\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\n",
    "\n",
    "# Step 10: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 11: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 12: Visualize Generator Output\n",
    "noise = np.random.randn(1000, latent_dim)\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(generated_data[:, 0], generated_data[:, 1], color='red', alpha=0.5, label=\"Generated Data\")\n",
    "plt.scatter(features['returns'], features['volatility'], color='blue', alpha=0.5, label=\"Real Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Generated vs Real Financial Data\")\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "abe577b8-c061-4756-9b85-b23aace583f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Epoch 1/10000\n",
      "Real Data Shape: (32, 3)\n",
      "Fake Data Shape: (32, 3)\n",
      "Real Labels Shape: (32, 1)\n",
      "Fake Labels Shape: (32, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m    148\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m--> 149\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Step 10: Apply Leland-Toft Model\u001b[39;00m\n\u001b[1;32m    152\u001b[0m lt_result \u001b[38;5;241m=\u001b[39m leland_toft_model(features)\n",
      "Cell \u001b[0;32mIn[76], line 117\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFake Labels Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfake_labels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Train the discriminator (real data and fake data)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_data, real_labels)\n\u001b[1;32m    118\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, fake_labels)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Train generator through GAN (flip labels, wants to fool discriminator)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m one_step_on_data(data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(step_function, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch financial data (e.g., stock prices) for a given ticker.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Step 2: Calculate derived financial features\n",
    "def calculate_features(data):\n",
    "    \"\"\"\n",
    "    Calculate financial features: returns, volatility, rate of change (ROC).\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = data.pct_change()\n",
    "\n",
    "    # Calculate rolling volatility (10-day window)\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "\n",
    "    # Calculate rate of change (ROC) over 5 days\n",
    "    roc = data.pct_change(periods=5)\n",
    "\n",
    "    # Merge into a DataFrame with aligned indexes (Automatically aligned)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "\n",
    "    # Drop rows where any column has NaN\n",
    "    features.dropna(inplace=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 3: Apply Leland-Toft Model (Leland Model for credit spread)\n",
    "def leland_toft_model(features):\n",
    "    \"\"\"\n",
    "    Apply Leland-Toft model to estimate credit spreads based on financial features.\n",
    "    \"\"\"\n",
    "    risk_free_rate = 0.03  # Example risk-free rate (3%)\n",
    "    asset_price = features['returns'].mean() * 100  # Assume a random asset price, can be real asset value\n",
    "    \n",
    "    volatility = features['volatility'].mean()  # Average volatility\n",
    "    sigma = volatility  # In this case, assume sigma is the volatility\n",
    "    \n",
    "    leverage = asset_price / (asset_price + sigma)  # Simple leverage assumption\n",
    "    default_spread = sigma * leverage  # Leland-Toft default spread formula\n",
    "    credit_spread = default_spread * (1 - risk_free_rate)  # Credit spread formula\n",
    "    \n",
    "    lt_results = {\n",
    "        \"default_spread\": default_spread,\n",
    "        \"credit_spread\": credit_spread,\n",
    "        \"leverage\": leverage\n",
    "    }\n",
    "\n",
    "    return lt_results\n",
    "\n",
    "# Step 4: Build and Compile the GAN model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator: Classifies if data is real or fake\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # Output: 1 (real/fake)\n",
    "    ])\n",
    "    \n",
    "    # Explicit compilation of discriminator\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator: Creates synthetic financial data\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # GAN: Stack generator and discriminator\n",
    "    discriminator.trainable = False  # We train the generator, discriminator stays frozen in GAN training\n",
    "    \n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    \n",
    "    # Explicitly compile GAN model with proper loss function\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Step 5: Train the GAN model with debugging\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    batch_count = features_values.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            # Train discriminator with a mix of real and fake data\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "            fake_data = generator.predict(np.random.randn(batch_size, latent_dim))\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))  # Real = 1\n",
    "            fake_labels = np.zeros((batch_size, 1))  # Fake = 0\n",
    "\n",
    "            # Print the shapes of data and labels for debugging\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            print(f\"Real Data Shape: {real_data.shape}\")\n",
    "            print(f\"Fake Data Shape: {fake_data.shape}\")\n",
    "            print(f\"Real Labels Shape: {real_labels.shape}\")\n",
    "            print(f\"Fake Labels Shape: {fake_labels.shape}\")\n",
    "\n",
    "            # Train the discriminator (real data and fake data)\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "            # Train generator through GAN (flip labels, wants to fool discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            g_loss = gan.train_on_batch(noise, real_labels)  # Generator wants the discriminator to classify as \"real\"\n",
    "\n",
    "            # Log the losses\n",
    "            print(f\"D Loss Real: {d_loss_real[0]:.4f}, D Loss Fake: {d_loss_fake[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Step 6: Fetch and Prepare Data\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Fetch financial data\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "\n",
    "# Step 7: Calculate features (returns, volatility, ROC)\n",
    "features = calculate_features(real_data)\n",
    "\n",
    "# Prepare input data for the GAN\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)  # Normalize data\n",
    "\n",
    "# Step 8: Build GAN model\n",
    "latent_dim = 10  # Latent space dimension (random noise)\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "\n",
    "# Step 9: Train the GAN\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\n",
    "\n",
    "# Step 10: Apply Leland-Toft Model\n",
    "lt_result = leland_toft_model(features)\n",
    "\n",
    "# Step 11: Print the Results\n",
    "print(\"Leland-Toft Model Results:\")\n",
    "print(f\"Default Spread: {lt_result['default_spread']:.6f}\")\n",
    "print(f\"Credit Spread: {lt_result['credit_spread']:.6f}\")\n",
    "print(f\"Leverage: {lt_result['leverage']:.4f}\")\n",
    "\n",
    "# Step 12: Visualize Generator Output\n",
    "noise = np.random.randn(1000, latent_dim)\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(generated_data[:, 0], generated_data[:, 1], color='red', alpha=0.5, label=\"Generated Data\")\n",
    "plt.scatter(features['returns'], features['volatility'], color='blue', alpha=0.5, label=\"Real Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Generated vs Real Financial Data\")\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0cc1d5-de9f-43cb-879b-299d17ec8eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 23:51:00.799474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737867060.966890  953724 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737867061.026027  953724 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-25 23:51:01.446011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-01-25 23:51:05.119852: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     77\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 78\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m     56\u001b[0m real_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     57\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 59\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_data, real_labels)\n\u001b[1;32m     60\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, fake_labels)\n\u001b[1;32m     62\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m one_step_on_data(data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(step_function, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Train the GAN Model\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    batch_count = features_values.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "            fake_data = generator.predict(np.random.randn(batch_size, latent_dim))\n",
    "\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, D Loss Real: {d_loss_real[0]:.4f}, D Loss Fake: {d_loss_fake[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a5f3b5b-14b1-4030-a675-79d32777639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x75249c2f9580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     82\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 83\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[80], line 59\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m     56\u001b[0m real_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     57\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 59\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_data, real_labels)\n\u001b[1;32m     60\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, fake_labels)\n\u001b[1;32m     62\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m one_step_on_data(data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(step_function, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Train the GAN Model\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    batch_count = features_values.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "            fake_data = generator.predict(np.random.randn(batch_size, latent_dim))\n",
    "\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, D Loss Real: {d_loss_real[0]:.4f}, D Loss Fake: {d_loss_fake[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "        # Save model checkpoints periodically\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            generator.save(f\"generator_epoch_{epoch + 1}.h5\")\n",
    "            discriminator.save(f\"discriminator_epoch_{epoch + 1}.h5\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25531aa7-f0eb-4272-87e9-6898950bb705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     80\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 81\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[82], line 62\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m     59\u001b[0m real_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     60\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 62\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(real_data, real_labels)\n\u001b[1;32m     63\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_data, fake_labels)\n\u001b[1;32m     65\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:598\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 598\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[1;32m    599\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:224\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m one_step_on_data(data)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(step_function, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[1;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    112\u001b[0m         outputs,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:66\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     60\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     61\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     67\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model using Functional API\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator\n",
    "    input_disc = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation='relu')(input_disc)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    output_disc = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = models.Model(input_disc, output_disc)\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator\n",
    "    input_gen = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(input_gen)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output_gen = layers.Dense(np.prod(input_shape), activation='tanh')(x)\n",
    "    output_gen_reshaped = layers.Reshape(input_shape)(output_gen)\n",
    "    generator = models.Model(input_gen, output_gen_reshaped)\n",
    "\n",
    "    # GAN model\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Train the GAN Model using functional API\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    batch_count = features_values.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "            fake_data = generator.predict(np.random.randn(batch_size, latent_dim))\n",
    "\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, D Loss Real: {d_loss_real[0]:.4f}, D Loss Fake: {d_loss_fake[0]:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ff19a00-4242-4cc2-b216-222298c4a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    104\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 105\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[84], line 74\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m     72\u001b[0m     d_loss_real \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mbinary_crossentropy(real_labels, real_pred)\n\u001b[1;32m     73\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(d_loss_real, discriminator\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 74\u001b[0m discriminator\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, discriminator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Train the discriminator on fake data\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:343\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m--> 343\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(grads, trainable_variables)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # GAN Model (Generator + Discriminator)\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Custom Training Loop with Explicit Loss Functions\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    # Create a custom training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for _ in range(features_values.shape[0] // batch_size):\n",
    "            # Create real data batch\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "\n",
    "            # Generate fake data\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            # Train the discriminator on real data\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_pred = discriminator(real_data)\n",
    "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_pred)\n",
    "            grads = tape.gradient(d_loss_real, discriminator.trainable_variables)\n",
    "            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "            # Train the discriminator on fake data\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_pred)\n",
    "            grads = tape.gradient(d_loss_fake, discriminator.trainable_variables)\n",
    "            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "\n",
    "            # Train the generator (we want to fool the discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_data = generator(noise)\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_pred)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "            generator.optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "        print(f\"Discriminator Loss (Real): {d_loss_real.numpy().mean():.4f}, Discriminator Loss (Fake): {d_loss_fake.numpy().mean():.4f}, Generator Loss: {g_loss.numpy().mean():.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0fd71726-1422-4bb2-b715-2afd4f18706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    119\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 120\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[86], line 103\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Check if grads are valid\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads):\n\u001b[0;32m--> 103\u001b[0m     generator\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, generator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Generator gradients are invalid or None!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'optimizer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # GAN Model (Generator + Discriminator)\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Custom Training Loop with Explicit Loss Functions\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    # Create a custom training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for _ in range(features_values.shape[0] // batch_size):\n",
    "            # Create real data batch\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "\n",
    "            # Generate fake data\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            # Train the discriminator on real data\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_pred = discriminator(real_data)\n",
    "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_pred)\n",
    "            grads = tape.gradient(d_loss_real, discriminator.trainable_variables)\n",
    "            \n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the discriminator on fake data\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_pred)\n",
    "            grads = tape.gradient(d_loss_fake, discriminator.trainable_variables)\n",
    "\n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the generator (we want to fool the discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_data = generator(noise)\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_pred)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "\n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                generator.optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Generator gradients are invalid or None!\")\n",
    "\n",
    "        print(f\"Discriminator Loss (Real): {d_loss_real.numpy().mean():.4f}, Discriminator Loss (Fake): {d_loss_fake.numpy().mean():.4f}, Generator Loss: {g_loss.numpy().mean():.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2a5d69a-bf05-413e-ab07-30441e82c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    122\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 123\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[88], line 106\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Check if grads are valid\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Manually apply gradients for generator\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     generator\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, generator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Generator gradients are invalid or None!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'optimizer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Generator\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # GAN Model (Generator + Discriminator)\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Custom Training Loop with Explicit Loss Functions\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    # Create a custom training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for _ in range(features_values.shape[0] // batch_size):\n",
    "            # Create real data batch\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "\n",
    "            # Generate fake data\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            # Train the discriminator on real data\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_pred = discriminator(real_data)\n",
    "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_pred)\n",
    "            grads = tape.gradient(d_loss_real, discriminator.trainable_variables)\n",
    "            \n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                # Manually apply gradients for discriminator\n",
    "                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the discriminator on fake data\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_pred)\n",
    "            grads = tape.gradient(d_loss_fake, discriminator.trainable_variables)\n",
    "\n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                # Manually apply gradients for discriminator\n",
    "                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the generator (we want to fool the discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_data = generator(noise)\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_pred)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "\n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                # Manually apply gradients for generator\n",
    "                generator.optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Generator gradients are invalid or None!\")\n",
    "\n",
    "        print(f\"Discriminator Loss (Real): {d_loss_real.numpy().mean():.4f}, Discriminator Loss (Fake): {d_loss_fake.numpy().mean():.4f}, Generator Loss: {g_loss.numpy().mean():.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91ce09b8-6d23-4f2c-ba68-95be0742ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    126\u001b[0m generator, discriminator, gan \u001b[38;5;241m=\u001b[39m build_gan(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, input_shape\u001b[38;5;241m=\u001b[39mfeatures_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 127\u001b[0m train_gan(generator, discriminator, gan, features_values, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n",
      "Cell \u001b[0;32mIn[90], line 110\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Check if grads are valid\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# Manually apply gradients for generator\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     generator\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, generator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Generator gradients are invalid or None!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'optimizer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "\n",
    "# Build the GAN Model\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Generator\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    # Compile discriminator separately\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # GAN Model (Generator + Discriminator)\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "\n",
    "    # Compile GAN with a dummy loss function since we don't train the discriminator here\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "# Custom Training Loop with Explicit Loss Functions\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    # Create a custom training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for _ in range(features_values.shape[0] // batch_size):\n",
    "            # Create real data batch\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "\n",
    "            # Generate fake data\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            # Train the discriminator on real data\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_pred = discriminator(real_data)\n",
    "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_pred)\n",
    "            grads = tape.gradient(d_loss_real, discriminator.trainable_variables)\n",
    "            \n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                # Manually apply gradients for discriminator\n",
    "                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the discriminator on fake data\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_pred)\n",
    "            grads = tape.gradient(d_loss_fake, discriminator.trainable_variables)\n",
    "\n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                # Manually apply gradients for discriminator\n",
    "                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the generator (we want to fool the discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_data = generator(noise)\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_pred)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "\n",
    "            # Check if grads are valid\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                # Manually apply gradients for generator\n",
    "                generator.optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Generator gradients are invalid or None!\")\n",
    "\n",
    "        print(f\"Discriminator Loss (Real): {d_loss_real.numpy().mean():.4f}, Discriminator Loss (Fake): {d_loss_fake.numpy().mean():.4f}, Generator Loss: {g_loss.numpy().mean():.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f3be1c-a5ee-48aa-ab1c-7c1693d30ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8395, Discriminator Loss (Fake): 0.6764, Generator Loss: 0.7134\n",
      "Epoch 2/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8333, Discriminator Loss (Fake): 0.6721, Generator Loss: 0.7113\n",
      "Epoch 3/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8291, Discriminator Loss (Fake): 0.6752, Generator Loss: 0.7106\n",
      "Epoch 4/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8496, Discriminator Loss (Fake): 0.6795, Generator Loss: 0.7073\n",
      "Epoch 5/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8286, Discriminator Loss (Fake): 0.6825, Generator Loss: 0.7023\n",
      "Epoch 6/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8637, Discriminator Loss (Fake): 0.6822, Generator Loss: 0.7035\n",
      "Epoch 7/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8526, Discriminator Loss (Fake): 0.6848, Generator Loss: 0.7008\n",
      "Epoch 8/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8360, Discriminator Loss (Fake): 0.6866, Generator Loss: 0.7009\n",
      "Epoch 9/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8400, Discriminator Loss (Fake): 0.6876, Generator Loss: 0.6998\n",
      "Epoch 10/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8566, Discriminator Loss (Fake): 0.6872, Generator Loss: 0.6985\n",
      "Epoch 11/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8277, Discriminator Loss (Fake): 0.6882, Generator Loss: 0.6981\n",
      "Epoch 12/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8503, Discriminator Loss (Fake): 0.6881, Generator Loss: 0.6992\n",
      "Epoch 13/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8332, Discriminator Loss (Fake): 0.6883, Generator Loss: 0.6991\n",
      "Epoch 14/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8273, Discriminator Loss (Fake): 0.6886, Generator Loss: 0.6977\n",
      "Epoch 15/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8467, Discriminator Loss (Fake): 0.6882, Generator Loss: 0.6969\n",
      "Epoch 16/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8402, Discriminator Loss (Fake): 0.6883, Generator Loss: 0.6974\n",
      "Epoch 17/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8502, Discriminator Loss (Fake): 0.6888, Generator Loss: 0.6976\n",
      "Epoch 18/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8566, Discriminator Loss (Fake): 0.6889, Generator Loss: 0.6969\n",
      "Epoch 19/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8469, Discriminator Loss (Fake): 0.6885, Generator Loss: 0.6971\n",
      "Epoch 20/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8252, Discriminator Loss (Fake): 0.6897, Generator Loss: 0.6969\n",
      "Epoch 21/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8352, Discriminator Loss (Fake): 0.6894, Generator Loss: 0.6971\n",
      "Epoch 22/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8195, Discriminator Loss (Fake): 0.6891, Generator Loss: 0.6976\n",
      "Epoch 23/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8445, Discriminator Loss (Fake): 0.6891, Generator Loss: 0.6974\n",
      "Epoch 24/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8427, Discriminator Loss (Fake): 0.6898, Generator Loss: 0.6974\n",
      "Epoch 25/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8161, Discriminator Loss (Fake): 0.6894, Generator Loss: 0.6966\n",
      "Epoch 26/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8526, Discriminator Loss (Fake): 0.6897, Generator Loss: 0.6970\n",
      "Epoch 27/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8560, Discriminator Loss (Fake): 0.6895, Generator Loss: 0.6962\n",
      "Epoch 28/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8192, Discriminator Loss (Fake): 0.6897, Generator Loss: 0.6961\n",
      "Epoch 29/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8346, Discriminator Loss (Fake): 0.6902, Generator Loss: 0.6959\n",
      "Epoch 30/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8355, Discriminator Loss (Fake): 0.6900, Generator Loss: 0.6971\n",
      "Epoch 31/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8693, Discriminator Loss (Fake): 0.6901, Generator Loss: 0.6959\n",
      "Epoch 32/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8470, Discriminator Loss (Fake): 0.6899, Generator Loss: 0.6960\n",
      "Epoch 33/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8226, Discriminator Loss (Fake): 0.6898, Generator Loss: 0.6965\n",
      "Epoch 34/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8335, Discriminator Loss (Fake): 0.6901, Generator Loss: 0.6963\n",
      "Epoch 35/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8451, Discriminator Loss (Fake): 0.6901, Generator Loss: 0.6968\n",
      "Epoch 36/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8136, Discriminator Loss (Fake): 0.6899, Generator Loss: 0.6962\n",
      "Epoch 37/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8477, Discriminator Loss (Fake): 0.6908, Generator Loss: 0.6959\n",
      "Epoch 38/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8434, Discriminator Loss (Fake): 0.6904, Generator Loss: 0.6959\n",
      "Epoch 39/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Discriminator Loss (Real): 0.8532, Discriminator Loss (Fake): 0.6901, Generator Loss: 0.6965\n",
      "Epoch 40/10000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Warning: Discriminator gradients are invalid or None!\n",
      "Warning: Discriminator gradients are invalid or None!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fetch financial data\n",
    "def fetch_financial_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return stock_data['Close']\n",
    "\n",
    "# Calculate financial features\n",
    "def calculate_features(data):\n",
    "    returns = data.pct_change()\n",
    "    volatility = returns.rolling(window=10).std()\n",
    "    roc = data.pct_change(periods=5)\n",
    "    features = pd.concat([returns, volatility, roc], axis=1)\n",
    "    features.columns = ['returns', 'volatility', 'roc']\n",
    "    features.dropna(inplace=True)\n",
    "    return features\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_gan(latent_dim, input_shape):\n",
    "    # Discriminator with LeakyReLU\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(128, input_shape=input_shape),\n",
    "        layers.LeakyReLU(alpha=0.2),  # Use LeakyReLU as an activation layer\n",
    "        layers.Dense(64),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n",
    "    ])\n",
    "    \n",
    "    # Generator\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(64, input_dim=latent_dim),\n",
    "        layers.ReLU(),\n",
    "        layers.Dense(128),\n",
    "        layers.ReLU(),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)  # Reshape to the desired output shape\n",
    "    ])\n",
    "    \n",
    "    # Compile the discriminator\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Compile the generator (Add this line)\n",
    "    generator.compile(optimizer='adam', loss='binary_crossentropy')  # Add optimizer here\n",
    "    \n",
    "    # GAN model (stacked generator and discriminator)\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    return generator, discriminator, gan\n",
    "\n",
    "\n",
    "# Build the GAN Model\n",
    "# Custom Training Loop with Gradient Clipping\n",
    "def train_gan(generator, discriminator, gan, features_values, epochs, batch_size, latent_dim):\n",
    "    # Create a custom training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for _ in range(features_values.shape[0] // batch_size):\n",
    "            # Create real data batch\n",
    "            real_data = features_values[np.random.randint(0, features_values.shape[0], batch_size)]\n",
    "\n",
    "            # Generate fake data\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            # Labels for real and fake data\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "            # Train the discriminator on real data\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_pred = discriminator(real_data)\n",
    "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_pred)\n",
    "            grads = tape.gradient(d_loss_real, discriminator.trainable_variables)\n",
    "\n",
    "            # Check and apply gradients with clipping\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                clipped_grads = [tf.clip_by_value(grad, -1.0, 1.0) for grad in grads]\n",
    "                discriminator.optimizer.apply_gradients(zip(clipped_grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the discriminator on fake data\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_pred)\n",
    "            grads = tape.gradient(d_loss_fake, discriminator.trainable_variables)\n",
    "\n",
    "            # Check and apply gradients with clipping\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                clipped_grads = [tf.clip_by_value(grad, -1.0, 1.0) for grad in grads]\n",
    "                discriminator.optimizer.apply_gradients(zip(clipped_grads, discriminator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Discriminator gradients are invalid or None!\")\n",
    "\n",
    "            # Train the generator (we want to fool the discriminator)\n",
    "            noise = np.random.randn(batch_size, latent_dim)\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_data = generator(noise)\n",
    "                fake_pred = discriminator(fake_data)\n",
    "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_pred)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "\n",
    "            # Check and apply gradients with clipping\n",
    "            if grads and all(grad is not None for grad in grads):\n",
    "                clipped_grads = [tf.clip_by_value(grad, -1.0, 1.0) for grad in grads]\n",
    "                generator.optimizer.apply_gradients(zip(clipped_grads, generator.trainable_variables))\n",
    "            else:\n",
    "                print(f\"Warning: Generator gradients are invalid or None!\")\n",
    "\n",
    "        print(f\"Discriminator Loss (Real): {d_loss_real.numpy().mean():.4f}, Discriminator Loss (Fake): {d_loss_fake.numpy().mean():.4f}, Generator Loss: {g_loss.numpy().mean():.4f}\")\n",
    "\n",
    "# Main Execution\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "real_data = fetch_financial_data(ticker, start_date, end_date)\n",
    "features = calculate_features(real_data)\n",
    "features_values = features.values\n",
    "features_values = (features_values - features_values.mean(axis=0)) / features_values.std(axis=0)\n",
    "\n",
    "latent_dim = 10\n",
    "generator, discriminator, gan = build_gan(latent_dim=latent_dim, input_shape=features_values.shape[1:])\n",
    "train_gan(generator, discriminator, gan, features_values, epochs=10000, batch_size=32, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3b4ed-5b3f-485f-baf6-85fec15b5cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
