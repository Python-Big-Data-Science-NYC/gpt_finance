{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be2db4-2bf1-4885-9c6a-3eb35a5a4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 00:27:09.749863: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736054829.785976  191621 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736054829.792756  191621 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-05 00:27:09.823460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/home/j/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-01-05 00:27:14.435304: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define latent dimension for the generator input\n",
    "latent_dim = 100\n",
    "\n",
    "# Fetch historical data for the 10-year U.S. Treasury yield (interest rate proxy)\n",
    "ticker = '^TNX'  # Symbol for the 10-year U.S. Treasury yield (in percentage)\n",
    "data = yf.download(ticker, start='2010-01-01', end='2024-01-01')\n",
    "\n",
    "# Use the 'Adj Close' as the interest rate (percentage)\n",
    "interest_rates = data['Adj Close']\n",
    "\n",
    "# Normalize the interest rates for GAN\n",
    "interest_rates_normalized = (interest_rates - interest_rates.mean()) / interest_rates.std()\n",
    "interest_rates_normalized = interest_rates_normalized.values.reshape(-1, 1)\n",
    "\n",
    "# Build the Generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='tanh'))  # Output is one interest rate value\n",
    "    return model\n",
    "\n",
    "# Build the Discriminator model\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(1,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Output probability of real or fake\n",
    "    return model\n",
    "\n",
    "# Build the GAN (combining generator and discriminator)\n",
    "def build_gan(generator, discriminator):\n",
    "    # Freeze discriminator when training the generator\n",
    "    discriminator.trainable = False\n",
    "    model = models.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Compile the models\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# The GAN model combines the generator and the discriminator\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10000\n",
    "half_batch = batch_size // 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a487a0-95b0-4751-9bad-a2d161afc225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Error during discriminator training: 'NoneType' object has no attribute 'update_state'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the latent dimension for the generator input\n",
    "latent_dim = 100\n",
    "\n",
    "# Build the Generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='tanh'))  # Output is a single value (e.g., a number)\n",
    "    return model\n",
    "\n",
    "# Build the Discriminator model\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(1,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Output probability of real or fake\n",
    "    return model\n",
    "\n",
    "# Build the GAN (combining the generator and the discriminator)\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False  # Freeze discriminator weights during GAN training\n",
    "    model = models.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Initialize the models\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# Create the GAN by combining the generator and discriminator\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10000\n",
    "half_batch = batch_size // 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Generate random data for real (normal distribution) and fake (from generator)\n",
    "    real_data = np.random.normal(0, 1, (half_batch, 1))  # Random \"real\" data (for simplicity)\n",
    "    \n",
    "    # Check if real_data contains NaN or Inf values\n",
    "    if np.any(np.isnan(real_data)) or np.any(np.isinf(real_data)):\n",
    "        print(\"Warning: real_data contains NaN or Inf\")\n",
    "        continue\n",
    "\n",
    "    # Noise for generator (input to generator)\n",
    "    noise = np.random.normal(0, 1, (half_batch, latent_dim))  # Random noise for the generator\n",
    "    fake_data = generator.predict(noise)  # Generate fake data from noise\n",
    "\n",
    "    # Ensure fake data is also valid\n",
    "    if np.any(np.isnan(fake_data)) or np.any(np.isinf(fake_data)):\n",
    "        print(\"Warning: fake_data contains NaN or Inf\")\n",
    "        continue\n",
    "\n",
    "    # Labels for real and fake data\n",
    "    real_labels = np.ones((half_batch, 1))  # Real data labeled as 1\n",
    "    fake_labels = np.zeros((half_batch, 1))  # Fake data labeled as 0\n",
    "\n",
    "    # Train the discriminator with real and fake data\n",
    "    try:\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during discriminator training: {e}\")\n",
    "        continue\n",
    "\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)  # Combine losses\n",
    "\n",
    "    # Train the generator via the GAN (the goal is to fool the discriminator into thinking fake data is real)\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))  # Generate random noise\n",
    "    valid_labels = np.ones((batch_size, 1))  # We want to fool the discriminator into thinking it's real\n",
    "\n",
    "    try:\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generator training: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Print progress every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {d_loss[1]*100}%] [G loss: {g_loss}]\")\n",
    "\n",
    "# Generate synthetic data after training\n",
    "noise = np.random.normal(0, 1, (1000, latent_dim))  # Generate synthetic samples\n",
    "synthetic_data = generator.predict(noise)\n",
    "\n",
    "# Rescale back to the range of real data (adjust as needed based on your data range)\n",
    "synthetic_data_rescaled = synthetic_data * 2  # Adjust as needed based on your data range\n",
    "\n",
    "# Plot real vs synthetic data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.random.normal(0, 1, (1000, 1)), label='Real Data')\n",
    "plt.plot(synthetic_data_rescaled, label='Generated Data', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b81a53-015c-4782-a937-cf26528a8d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
