{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb0ad69-0683-4532-a760-652091560544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning BERT with a Logistic Regression Layer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1ccfec-4b94-4a94-b6d9-caa156814d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91be2fed-3710-4c43-a896-ad1dfcccd607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b761887-e18b-4964-addb-381f577c5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.47\n",
      "Test Accuracy: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 49/100 03:28 < 03:46, 0.23 it/s, Epoch 1.92/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Balance the dataset to include both classes\n",
    "def balance_classes(dataset, num_samples):\n",
    "    class_0 = [sample for sample in dataset if sample[\"label\"] == 0][:num_samples // 2]\n",
    "    class_1 = [sample for sample in dataset if sample[\"label\"] == 1][:num_samples // 2]\n",
    "    return class_0 + class_1\n",
    "\n",
    "balanced_train_data = balance_classes(tokenized_datasets[\"train\"], 100)\n",
    "balanced_eval_data = balance_classes(tokenized_datasets[\"test\"], 50)\n",
    "\n",
    "# Define the training arguments with optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # Output directory\n",
    "    evaluation_strategy=\"no\",               # Disable evaluation during training\n",
    "    learning_rate=2e-5,                     # Learning rate\n",
    "    per_device_train_batch_size=4,          # Smaller batch size for faster training\n",
    "    per_device_eval_batch_size=4,           # Smaller batch size for evaluation\n",
    "    num_train_epochs=1,                     # Reduce number of epochs to 1\n",
    "    fp16=True,                              # Enable mixed precision training\n",
    "    max_steps=100,                          # Limit steps for faster execution\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Function to extract embeddings (logits) from BERT\n",
    "def extract_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:  # Loop through each item in the dataset\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            embeddings.append(logits.flatten())  # Append flattened logits\n",
    "            labels.append(sample[\"label\"])  # Append label\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Extract embeddings from the train and test datasets\n",
    "train_embeddings, train_labels = extract_embeddings(model, balanced_train_data)\n",
    "test_embeddings, test_labels = extract_embeddings(model, balanced_eval_data)\n",
    "\n",
    "# Train a logistic regression model on top of the embeddings\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Predict and evaluate\n",
    "train_preds = log_reg.predict(train_embeddings)\n",
    "test_preds = log_reg.predict(test_embeddings)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f3295-d6a9-4ec9-8731-338d774a2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer for BERT\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=balanced_train_data,\n",
    "    eval_dataset=balanced_eval_data,\n",
    ")\n",
    "\n",
    "# Train the model (BERT fine-tuning)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(pred)\n",
    "            labels.extend(sample[\"label\"])\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on the test set\n",
    "initial_accuracy = evaluate_model(model, balanced_eval_data)\n",
    "print(f\"Accuracy before Logistic Regression: {initial_accuracy}\")\n",
    "\n",
    "# Save the trained BERT model and tokenizer for future use\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b422c-5452-40e0-a92b-511780ddee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predict using the fine-tuned BERT model\n",
    "def predict(texts, model, tokenizer):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.numpy()\n",
    "\n",
    "# Example prediction\n",
    "texts = [\"I love this movie!\", \"This movie was terrible.\"]\n",
    "predictions = predict(texts, model, tokenizer)\n",
    "print(\"Predictions:\", predictions)  # Output will be the predicted labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c49ae33-3b49-4739-9b58-d68a72a2c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x737c456f6ba0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffb33bc-807d-4b45-8417-d2439580a450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef8773e-e0bc-43fd-ae32-2331acd743f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love this movie!', 'This movie was terrible.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1aa51a-892e-4ac1-bf87-6d07d1631281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5184704f-2631-48f2-a279-eedfe2fc5ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fbc0cb-a5b1-43d4-9569-feda8f660bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the fine-tuned BERT model\n",
    "def predict(texts, model, tokenizer):\n",
    "    # Encode the input texts using the tokenizer\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model to get logits\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to probabilities using softmax (optional but useful for multi-class)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get the predicted class (index of the highest probability)\n",
    "        predictions = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    return predictions.numpy()\n",
    "\n",
    "# Example prediction\n",
    "texts = [\"I love this movie!\", \"This movie was terrible.\"]\n",
    "predictions = predict(texts, model, tokenizer)\n",
    "print(\"Predictions:\", predictions)  # Output should show the predicted labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0cdbe1-823b-437f-b795-6d7f487bf880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "texts = [\n",
    "    \"I love this movie! It's amazing and so entertaining.\",\n",
    "    \"This movie was terrible. I hated every minute of it.\",\n",
    "    \"The plot was great, but the acting could have been better.\",\n",
    "    \"A fantastic film with breathtaking visuals and a captivating story!\",\n",
    "    \"I was really bored throughout this film. It's not worth watching.\",\n",
    "    \"One of the best movies Iâ€™ve seen this year. I highly recommend it!\",\n",
    "    \"The movie started off strong but ended poorly.\",\n",
    "    \"Not bad, but not great either. Just an average movie.\",\n",
    "    \"I couldn't stop laughing at the jokes. This was a really funny film.\",\n",
    "    \"It was too predictable and lacked depth. Would not watch again.\",\n",
    "    \"What a disappointment! The trailer was way better than the actual movie.\",\n",
    "    \"Iâ€™m glad I watched it, but I wouldnâ€™t watch it again.\",\n",
    "    \"Incredible! A masterpiece that will be remembered for years.\",\n",
    "    \"This movie is a must-see for any fan of action films.\",\n",
    "    \"Absolutely awful. I donâ€™t understand the hype around it.\",\n",
    "    \"The soundtrack was amazing, and the acting was top-notch.\",\n",
    "    \"This was a complete waste of time. I would not recommend it to anyone.\",\n",
    "    \"The movie was too long and dragged on. Could have been much shorter.\",\n",
    "    \"A rollercoaster of emotions. I loved every second of it.\",\n",
    "    \"Itâ€™s a fun movie to watch with friends, but not the best out there.\",\n",
    "    \"A very emotional and touching story that left me in tears.\",\n",
    "    \"The special effects were spectacular, but the story was lacking.\",\n",
    "    \"I didnâ€™t connect with the characters at all, but the film was well made.\",\n",
    "    \"If you're looking for something lighthearted, this movie is perfect.\",\n",
    "    \"A complete masterpiece from start to finish. I loved everything about it!\",\n",
    "    \"This movie is definitely overrated. I donâ€™t see what the fuss is about.\"\n",
    "]\n",
    "\n",
    "# Predictions\n",
    "predictions = predict(texts, model, tokenizer)\n",
    "print(\"Predictions:\", predictions)  # Output will be the predicted labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "216f6773-d9bd-4e1e-9a44-9438f24d79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The training data contains samples from only one class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m unique_train_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(train_labels)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_train_labels) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe training data contains samples from only one class.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 6. Apply feature selection (SelectKBest)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Select top 50 features for faster processing\u001b[39;00m\n\u001b[1;32m     52\u001b[0m selector \u001b[38;5;241m=\u001b[39m SelectKBest(score_func\u001b[38;5;241m=\u001b[39mf_classif, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The training data contains samples from only one class."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load a smaller dataset (a subset of IMDB for faster processing)\n",
    "dataset = load_dataset(\"imdb\", split='train[:5%]')  # Use only 5% of the data for faster training\n",
    "test_dataset = load_dataset(\"imdb\", split='test[:5%]')  # Use only 5% of the test data\n",
    "\n",
    "# 2. Load a smaller model (DistilBERT)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 3. Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Extract embeddings (features) from DistilBERT\n",
    "def extract_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            embeddings.append(logits.flatten())  # Append flattened logits\n",
    "            labels.append(sample[\"label\"])  # Append label\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Extract embeddings from train and test datasets\n",
    "train_embeddings, train_labels = extract_embeddings(model, tokenized_datasets)\n",
    "test_embeddings, test_labels = extract_embeddings(model, tokenized_test_data)\n",
    "\n",
    "# 5. Ensure both classes are present before applying feature selection\n",
    "# Check if there are both classes in the data\n",
    "unique_train_labels = np.unique(train_labels)\n",
    "if len(unique_train_labels) < 2:\n",
    "    raise ValueError(\"The training data contains samples from only one class.\")\n",
    "\n",
    "# 6. Apply feature selection (SelectKBest)\n",
    "# Select top 50 features for faster processing\n",
    "selector = SelectKBest(score_func=f_classif, k=50)\n",
    "train_embeddings_selected = selector.fit_transform(train_embeddings, train_labels)\n",
    "test_embeddings_selected = selector.transform(test_embeddings)\n",
    "\n",
    "# 7. Identify the selected and removed features\n",
    "selected_features = selector.get_support(indices=True)  # Indices of selected features\n",
    "removed_features = [i for i in range(train_embeddings.shape[1]) if i not in selected_features]\n",
    "\n",
    "# Display the selected and removed features\n",
    "print(\"Selected feature indices:\", selected_features)\n",
    "print(\"Removed feature indices:\", removed_features)\n",
    "\n",
    "# 8. Train a Logistic Regression model on the selected features\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(train_embeddings_selected, train_labels)\n",
    "\n",
    "# 9. Evaluate the model\n",
    "train_preds = log_reg.predict(train_embeddings_selected)\n",
    "test_preds = log_reg.predict(test_embeddings_selected)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af5229-043f-4e63-aa12-88fd6223cdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
