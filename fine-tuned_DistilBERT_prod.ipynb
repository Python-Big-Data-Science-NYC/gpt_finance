{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcdf493-62a1-40be-9836-f5c4e980d35b",
   "metadata": {},
   "source": [
    " fine-tuned_DistilBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c679b70e-d948-4e0b-bb83-681e136f9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"The stock market is showing signs of recovery.\"\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Text: \"Unexpected economic downturn affects financial stability.\"\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Text: \"Major company files for bankruptcy.\"\n",
      "Predicted Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries if not already installed\n",
    "# !pip install transformers torch datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "# Load the fine-tuned DistilBERT model and tokenizer\n",
    "# MODEL_NAME = \"shivgan3/shivgan-fine-tuned-model-name\"  # Replace with your fine-tuned model's name or path\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,  model_type=\"bert\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  # Or use any other model from the Hugging Face Model Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Define the text inputs for classification\n",
    "texts = [\n",
    "    \"The stock market is showing signs of recovery.\",\n",
    "    \"Unexpected economic downturn affects financial stability.\",\n",
    "    \"Major company files for bankruptcy.\"\n",
    "]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Define label mapping (adjust this to match your fine-tuned model's labels)\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "# Map predictions to labels\n",
    "classified_labels = [label_map[pred.item()] for pred in predictions]\n",
    "\n",
    "# Print results\n",
    "for text, label in zip(texts, classified_labels):\n",
    "    print(f\"Text: \\\"{text}\\\"\\nPredicted Sentiment: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84eaa9-c47d-4ff0-a055-0518333adf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Replace with the correct model or base model name\n",
    "model_name = \"shivgan3/shivgan-fine-tuned-model-name\"\n",
    "\n",
    "try:\n",
    "    # Attempt to load tokenizer from the given model name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer loaded successfully!\")\n",
    "except OSError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Attempting to use a base model tokenizer as fallback...\")\n",
    "\n",
    "    # Fallback to a base BERT tokenizer if the custom tokenizer cannot be loaded\n",
    "    base_model = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    print(f\"Fallback to base model tokenizer ({base_model}) successful!\")\n",
    "\n",
    "# Test the tokenizer\n",
    "test_sentence = \"This is a test sentence for the tokenizer.\"\n",
    "tokenized_output = tokenizer.tokenize(test_sentence)\n",
    "print(\"Tokenized output:\", tokenized_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59826a21-805f-4d20-9e13-b9db5585cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory ./shivgan3_shivgan_fine_tuned_model does not exist. Please download and save the model from Hugging Face manually.\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries if not already installed\n",
    "# !pip install transformers torch datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set local directory for saving the model and tokenizer\n",
    "MODEL_DIR = \"./shivgan3_shivgan_fine_tuned_model\"  # Path to your local model directory\n",
    "\n",
    "# Ensure the model and tokenizer are downloaded\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    print(f\"Model directory {MODEL_DIR} does not exist. Please download and save the model from Hugging Face manually.\")\n",
    "else:\n",
    "    # Load the fine-tuned model and tokenizer from local files\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    # Define the text inputs for classification\n",
    "    texts = [\n",
    "        \"The stock market is showing signs of recovery.\",\n",
    "        \"Unexpected economic downturn affects financial stability.\",\n",
    "        \"Major company files for bankruptcy.\"\n",
    "    ]\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Define label mapping (adjust this to match your fine-tuned model's labels)\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "    # Map predictions to labels\n",
    "    classified_labels = [label_map[pred.item()] for pred in predictions]\n",
    "\n",
    "    # Print results\n",
    "    for text, label in zip(texts, classified_labels):\n",
    "        print(f\"Text: \\\"{text}\\\"\\nPredicted Sentiment: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c40117b-2f69-417e-93d1-9a15cea69cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving the model to ./my_local_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./my_local_model\n",
      "Text: \"The stock market is showing signs of recovery.\"\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Text: \"Unexpected economic downturn affects financial stability.\"\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Text: \"Major company files for bankruptcy.\"\n",
      "Predicted Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries if not already installed\n",
    "# !pip install transformers torch datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the model name (pre-trained model of your choice)\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Replace with your chosen pre-trained model\n",
    "\n",
    "# Set the local directory for saving the model and tokenizer\n",
    "MODEL_DIR = \"./my_local_model\"  # Path to save model locally\n",
    "\n",
    "# Step 1: Download and save the model and tokenizer locally if not already saved\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    print(f\"Downloading and saving the model to {MODEL_DIR}...\")\n",
    "    # Load the tokenizer and model from Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Save the model and tokenizer locally\n",
    "    tokenizer.save_pretrained(MODEL_DIR)\n",
    "    model.save_pretrained(MODEL_DIR)\n",
    "    print(f\"Model saved to {MODEL_DIR}\")\n",
    "else:\n",
    "    # Step 2: Load the model and tokenizer from the local directory\n",
    "    print(f\"Loading model from local directory {MODEL_DIR}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Define the text inputs for classification\n",
    "texts = [\n",
    "    \"The stock market is showing signs of recovery.\",\n",
    "    \"Unexpected economic downturn affects financial stability.\",\n",
    "    \"Major company files for bankruptcy.\"\n",
    "]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Define label mapping (adjust this to match your fine-tuned model's labels)\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}  # Adjust based on your model's labels\n",
    "\n",
    "# Map predictions to labels\n",
    "classified_labels = [label_map[pred.item()] for pred in predictions]\n",
    "\n",
    "# Print results\n",
    "for text, label in zip(texts, classified_labels):\n",
    "    print(f\"Text: \\\"{text}\\\"\\nPredicted Sentiment: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e3ea2-6e23-4c1c-a9c3-191fca421724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
